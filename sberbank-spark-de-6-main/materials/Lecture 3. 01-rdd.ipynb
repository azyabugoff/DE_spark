{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Введение в Spark: RDD API\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Общие сведения\n",
    "+ Область применения\n",
    "+ Архитектура приложений\n",
    "+ Базовые функции RDD API\n",
    "+ Рair RDD функции\n",
    "+ Работа с данными\n",
    "\n",
    "## Общие сведения\n",
    "\n",
    "**Apache Spark** - это:\n",
    "+ Платформа для построения распределнных приложений обработки данных\n",
    "+ Эволюция Hadoop MapReduce\n",
    "+ Библиотека под Python/Scala\n",
    "+ Один из самых популярных проектов в области обработки больших данных\n",
    "\n",
    "\n",
    "## Область применения\n",
    "- Распределенная обработка больших данных\n",
    "- Построение ETL пайплайнов\n",
    "- Работа со структурированными данными (SQL)\n",
    "- Разработка стриминг приложений\n",
    "\n",
    "## Архитектура приложения\n",
    "\n",
    "+ **Driver** (aka Master):\n",
    "  - предоставляет API через SparkSession и SparkContext\n",
    "  - выполняет ваш код - python файл или скомпилированный .jar\n",
    "  - контролирует выполнение задачи\n",
    "\n",
    "+ **Workers** (aka Executors or Slaves):\n",
    "  - обрабатывают данные\n",
    "  - каждый Worker работает со своим сегментом данных - **Partition**\n",
    "  - не выполняются ваш код напрямую\n",
    "  - получают задачи от Driver\n",
    "+ **Cluster Manager** (YARN/Mesos):\n",
    "  - отвечает за аллокацию контейнеров, выполняющих код драйвера и воркеров, на кластере\n",
    "  - квотирует ресурсы между пользователями\n",
    "  - контролирует состояние контейнеров\n",
    " \n",
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-06-05%2013.07.01.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset\n",
    "\n",
    "**RDD** aka Resilient Distributet Dataset - самая базовая и самая низкоуровневая структура в Spark, доступная разработчику. Представляет собой типизированную неизменяемую неупорядоченную партиционированную коллекцию данных, распределенную по узлам кластера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cities: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")\n",
    "println(s\"The Vector has ${cities.length} elements, the first one is ${cities.head}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val rdd = sc.parallelize(cities)\n",
    "println(s\"The RDD has ${rdd.count} elements, the first one is ${rdd.first}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD может быть создана из:\n",
    "- локальной коллекции на драйвере\n",
    "- файла (локального или на распределенной файловой системе, например HDFS)\n",
    "- базы данных\n",
    "\n",
    "### Операции с RDD\n",
    "1. Трансформации (e.g. map, filter)\n",
    "2. Действия (e.g. reduce, collect, count, foreach)\n",
    "\n",
    "**Трансформации** (Transormations):\n",
    "- всегда превращают один RDD в новый RDD\n",
    "- всегда являются ленивыми - создают граф вычислений, но не запускают их\n",
    "- иногда (часто) неявно требуют перемешивания данных между воркерами - **Shuffle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Трансформация map: не запускает вычислений, не изменяет изначальный RDD\n",
    "val upperRdd: RDD[String] = rdd.map ( city => city.toUpperCase )\n",
    "\n",
    "\n",
    "// Метод take возвращает N первых элементов RDD\n",
    "val upperVec = upperRdd.take(3)\n",
    "val oldVec = rdd.take(3)\n",
    "\n",
    "// метод mkString позволяет сделать из любой локальной коллекции строку\n",
    "println(s\"\"\"New RDD: ${upperVec.mkString(\", \")}\"\"\")\n",
    "println(s\"\"\"Old RDD: ${oldVec.mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.15.06.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Действия** (Actions):\n",
    "- выполняют действие над RDD\n",
    "- запускают вычисления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Действие reduce применяет функцию f к промежуточному результату \n",
    "// от предыдущей итерации со следующим элементом коллекции\n",
    "val count = rdd.map( x => x.length ).reduce { (x,y) => x + y }\n",
    "println(s\"The RDD contains ${count} letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.22.10.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры операций с RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтрация RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val startsWithM = upperRdd.filter( x => x.startsWith(\"M\") )\n",
    "println(s\"\"\"The following city names starts with M: ${startsWithM.take(2).mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.33.22.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет количества элементов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countM: Long = startsWithM.count()\n",
    "println(s\"The RDD contains $countM elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.45.03.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передача ВСЕХ элементов RDD на драйвер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val localArray: Array[String] = startsWithM.collect()\n",
    "val containsMoscow: Boolean = localArray.contains(\"MOSCOW\")\n",
    "println(s\"The array contains MOSCOW: $containsMoscow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передача N элементов по сети на драйвер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val twoElements: Array[String] = startsWithM.take(2)\n",
    "println(s\"\"\"Two elements of the RDD are: ${twoElements.mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортировка и выборка из N первых элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val twoElementsSorted: Array[String] = startsWithM.takeOrdered(2)\n",
    "println(s\"\"\"Two elements of the RDD are: ${twoElementsSorted.mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2020.40.09.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спрямление вложенных коллекций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mappedRdd: RDD[Vector[Char]] = rdd.map(x => x.toVector)\n",
    "mappedRdd.take(2).foreach(println)\n",
    "println(\"####\")\n",
    "\n",
    "val flatMappedRdd = rdd.flatMap( x => x.toLowerCase )\n",
    "flatMappedRdd.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val uniqueLetters: String = \n",
    "    flatMappedRdd\n",
    "        .distinct\n",
    "        .filter(x => x != ' ')\n",
    "        .collect\n",
    "        .sorted\n",
    "        .mkString(\" \")\n",
    "\n",
    "println(s\"Letters in the RDD are: ${uniqueLetters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2020.42.42.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- RDD - это неизменяемый распределенный набор данных\n",
    "- Трансформации (map, filter, flatMap) создают новый RDD из существующего и не изменяют существующий\n",
    "- Любые трансформации являются ленивыми и не запускают вычислений\n",
    "- Действия (count, reduce, collect, take) запускают вычисления\n",
    "\n",
    "### Полезные ссылки:\n",
    "- [RDD API Reference](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)\n",
    "- [RDD Programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [Scala 2.11.12 API](https://www.scala-lang.org/files/archive/api/2.11.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairRDD функции\n",
    "\n",
    "Во всех вышестоящих экспериментах мы создавали RDD, состоящие из элементов базовых типов - числовых, срок и символов. На самом деле RDD не имеют как таковых органичений на тип элементов. Ими могут выступать коллекции, кейс \n",
    "классы, кортежи и т.д.\n",
    "\n",
    "**PairRDD** - расширенный класс функций, доступных для RDD, где элементы - это кортеж (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pairRdd: RDD[(Char, Int)] = rdd.flatMap( x => x.toLowerCase ).map( x => (x, 1))\n",
    "pairRdd.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countByKey подсчитывает количество кортежей по каждому ключу и возвращает локальный Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val letterCount: scala.collection.Map[Char,Long] = pairRdd.countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey работает аналогично обычному reduce, но промежуточный итог накапливается по каждому ключу независимо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val letterCount: RDD[(Char, Int)] = pairRdd.reduceByKey { (x,y) => x + y }\n",
    "println(letterCount.take(3).mkString(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2020.57.17.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join позволяет соединить два RDD по ключу. Поддерживаются join, leftOuterJoin и fullOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val favouriteLetters: Vector[Char] = Vector('a', 'd', 'o')\n",
    "val favLetRdd = sc.parallelize(favouriteLetters).map(x => (x,1))\n",
    "\n",
    "\n",
    "val joined: RDD[(Char, (Int, Option[Int]))] = letterCount.leftOuterJoin(favLetRdd)\n",
    "joined.collect.foreach { j => \n",
    "    val (letter, (leftCount, rightCount)) = j\n",
    "    rightCount match {\n",
    "        case Some(v) => println(s\"The letter $letter is my favourite and it appears in the RDD $leftCount times\")\n",
    "        case None => println(s\"The letter $letter is not my favourite!\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- PairRDD функции - расширенный список функций, доступный для RDD, элементы которых являются кортежем (K, V)\n",
    "- PairRDD позволяют соединять два RDD по ключу K\n",
    "\n",
    "### Полезные ссылки:\n",
    "- [PairRDD API Reference](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с данными\n",
    "\n",
    "Для изучения структуры и вычислений RDD проведем анализ датасета [Airport Codes](https://datahub.io/core/airport-codes)  \n",
    "\n",
    "Метод `sc.textFile` позволяет прочитать файл на локальной, S3 или HDFS совместимой ФС. С помощью данного метода можно читать как обычные файлы, так и директории с файлами, а также архивы с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.textFile(\"/tmp/datasets/airport-codes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем первые 3 строки на экран:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим `case class` для парсинга данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Airport(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevationFt: String,\n",
    "    continent: String,\n",
    "    isoCountry: String,\n",
    "    isoRegion: String,\n",
    "    municipality: String,\n",
    "    gpsCode: String,\n",
    "    iataCode: String,\n",
    "    localCode: String,\n",
    "    longitude: String,\n",
    "    latitude: String\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем шапку и ненужные кавычки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val firstElem = rdd.first\n",
    "\n",
    "val noHeader = rdd.filter(x => x != firstElem).map(x => x.replaceAll(\"\\\"\", \"\"))\n",
    "noHeader.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая преобразует `RDD[String] => RDD[Airport]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toAirport(data: String): Airport = {\n",
    "    val airportArr: Array[String] = data.split(\",\").map(_.trim)\n",
    "    val Array(\n",
    "        ident, \n",
    "        aType, \n",
    "        name, \n",
    "        elevationFt, \n",
    "        continent, \n",
    "        isoCountry, \n",
    "        isoRegion, \n",
    "        municipality, \n",
    "        gpsCode, \n",
    "        iataCode, \n",
    "        localCode, \n",
    "        longitude,\n",
    "        latitude) = airportArr\n",
    "    \n",
    "    Airport(\n",
    "        ident = ident,\n",
    "        `type` = aType,\n",
    "        name = name,\n",
    "        elevationFt = elevationFt,\n",
    "        continent = continent,\n",
    "        isoCountry = isoCountry,\n",
    "        isoRegion = isoRegion,\n",
    "        municipality = municipality,\n",
    "        gpsCode = gpsCode,\n",
    "        iataCode = iataCode,\n",
    "        localCode = localCode,\n",
    "        longitude = longitude,\n",
    "        latitude = latitude\n",
    "    )\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним преобразование RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportRdd: RDD[Airport] = noHeader.map(x => toAirport(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку любые трансформации являются ленивыми, отсутствие ошибок при выполнении предыдущей ячейки еще не означает, что данная функция отрабатывает корректно на всем датасете. Проверим это с помощью операции `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportRdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что произошло? `count`, как и любой action, запускает вычисление всех элементов в RDD. Если посмотреть стектрейс, мы увидим причину возникновения ошибки: \n",
    "\n",
    "`Caused by: scala.MatchError: [Ljava.lang.String;@42ee14f9 (of class [Ljava.lang.String;)`\n",
    "\n",
    "Это означает, что размер массива, полученного после операции `split`, меньше количества переменных, которые мы указали в данной операции:\n",
    "\n",
    "```scala\n",
    "val Array(\n",
    "        ident, \n",
    "        aType, \n",
    "        name, \n",
    "        elevationFt, \n",
    "        continent, \n",
    "        isoCountry, \n",
    "        isoRegion, \n",
    "        municipality, \n",
    "        gpsCode, \n",
    "        iataCode, \n",
    "        localCode, \n",
    "        longitude,\n",
    "        latitude) = airportArr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим код функции `toAirport`, чтобы решить данную проблему. Для простоты будем считать, что если в строке недостаточно элементов, то эту строку следует выкинуть (сделать `None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toAirportOpt(data: String): Option[Airport] = {\n",
    "    val airportArr: Array[String] = data.split(\",\", -1)\n",
    "    \n",
    "    airportArr match {\n",
    "        case Array(\n",
    "            ident, \n",
    "            aType, \n",
    "            name, \n",
    "            elevationFt, \n",
    "            continent, \n",
    "            isoCountry, \n",
    "            isoRegion, \n",
    "            municipality, \n",
    "            gpsCode, \n",
    "            iataCode, \n",
    "            localCode, \n",
    "            longitude,\n",
    "            latitude) => {\n",
    "        \n",
    "                Some(\n",
    "                    Airport(\n",
    "                        ident = ident,\n",
    "                        `type` = aType,\n",
    "                        name = name,\n",
    "                        elevationFt = elevationFt,\n",
    "                        continent = continent,\n",
    "                        isoCountry = isoCountry,\n",
    "                        isoRegion = isoRegion,\n",
    "                        municipality = municipality,\n",
    "                        gpsCode = gpsCode,\n",
    "                        iataCode = iataCode,\n",
    "                        localCode = localCode,\n",
    "                        longitude = longitude,\n",
    "                        latitude = latitude\n",
    "                        )\n",
    "                    )\n",
    "        }\n",
    "        case _ => Option.empty[Airport]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим новую функцию к RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportOptRdd: RDD[Option[Airport]] = noHeader.map(toAirportOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим корректность выполнения функции на первых трех элементах и на всем датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportOptRdd.take(3).foreach(println)\n",
    "airportOptRdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportRdd: RDD[Airport] = noHeader.flatMap(toAirportOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportRdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим корректную обработку числовых типов в наш код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class AirportTyped(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevationFt: Int,\n",
    "    continent: String,\n",
    "    isoCountry: String,\n",
    "    isoRegion: String,\n",
    "    municipality: String,\n",
    "    gpsCode: String,\n",
    "    iataCode: String,\n",
    "    localCode: String,\n",
    "    longitude: Float,\n",
    "    latitude: Float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toAirportOptTyped(data: String): Option[AirportTyped] = {\n",
    "    val airportArr: Array[String] = data.split(\",\", -1)\n",
    "    \n",
    "    airportArr match {\n",
    "        case Array(\n",
    "            ident, \n",
    "            aType, \n",
    "            name, \n",
    "            elevationFt, \n",
    "            continent, \n",
    "            isoCountry, \n",
    "            isoRegion, \n",
    "            municipality, \n",
    "            gpsCode, \n",
    "            iataCode, \n",
    "            localCode, \n",
    "            longitude,\n",
    "            latitude) => {\n",
    "        \n",
    "                Some(\n",
    "                    AirportTyped(\n",
    "                        ident = ident,\n",
    "                        `type` = aType,\n",
    "                        name = name,\n",
    "                        elevationFt = elevationFt.toInt,\n",
    "                        continent = continent,\n",
    "                        isoCountry = isoCountry,\n",
    "                        isoRegion = isoRegion,\n",
    "                        municipality = municipality,\n",
    "                        gpsCode = gpsCode,\n",
    "                        iataCode = iataCode,\n",
    "                        localCode = localCode,\n",
    "                        longitude = longitude.toFloat,\n",
    "                        latitude = latitude.toFloat\n",
    "                        )\n",
    "                    )\n",
    "        }\n",
    "        case _ => Option.empty[AirportTyped]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportRddTyped: RDD[AirportTyped] = noHeader.flatMap(toAirportOptTyped)\n",
    "airportRddTyped.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас новая ошибка:\n",
    "\n",
    "`java.lang.NumberFormatException: For input string: \"\"`\n",
    "\n",
    "Она возникает, когда мы пытаемся превратить пустую строку в Int с помощью метода `.toInt`\n",
    "Для решения этой задачи мы можем использовать монаду `Try[T]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Try\n",
    "\n",
    "case class AirportSafe(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevationFt: Option[Int],\n",
    "    continent: String,\n",
    "    isoCountry: String,\n",
    "    isoRegion: String,\n",
    "    municipality: String,\n",
    "    gpsCode: String,\n",
    "    iataCode: String,\n",
    "    localCode: String,\n",
    "    longitude: Option[Float],\n",
    "    latitude: Option[Float]\n",
    ")\n",
    "\n",
    "def toAirportOptSafe(data: String): Option[AirportSafe] = {\n",
    "    val airportArr: Array[String] = data.split(\",\", -1)\n",
    "    \n",
    "    airportArr match {\n",
    "        case Array(\n",
    "            ident, \n",
    "            aType, \n",
    "            name, \n",
    "            elevationFt, \n",
    "            continent, \n",
    "            isoCountry, \n",
    "            isoRegion, \n",
    "            municipality, \n",
    "            gpsCode, \n",
    "            iataCode, \n",
    "            localCode, \n",
    "            longitude,\n",
    "            latitude) => {\n",
    "        \n",
    "                Some(\n",
    "                    AirportSafe(\n",
    "                        ident = ident,\n",
    "                        `type` = aType,\n",
    "                        name = name,\n",
    "                        elevationFt = Try(elevationFt.toInt).toOption,\n",
    "                        continent = continent,\n",
    "                        isoCountry = isoCountry,\n",
    "                        isoRegion = isoRegion,\n",
    "                        municipality = municipality,\n",
    "                        gpsCode = gpsCode,\n",
    "                        iataCode = iataCode,\n",
    "                        localCode = localCode,\n",
    "                        longitude = Try(longitude.toFloat).toOption,\n",
    "                        latitude = Try(latitude.toFloat).toOption\n",
    "                        )\n",
    "                    )\n",
    "        }\n",
    "        case _ => Option.empty[AirportSafe]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим данную функцию к нашему датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportSafeRdd: RDD[Option[AirportSafe]] = noHeader.map(toAirportOptSafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим ее применимость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportSafeRdd.take(3).foreach(println)\n",
    "airportSafeRdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Option[T]` - это удобная монада, которая позволяет работать с отсутствующими данными, избегая исключительных ситуаций и обработки `null`. Одним из ее преимуществ является то, что ее можно рассматривать как коллекцию, что позволяет применить к `RDD[Option[T]]` метод `flatMap`, который вернет RDD[T], убрав все `None` из нашего датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportFinal: RDD[AirportSafe] = noHeader.flatMap(toAirportOptSafe)\n",
    "airportFinal.take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим коллекцию, содержащую максимальную высота аэропорта с разбивкой по странам. Для этого первым шагом получим `PairRDD`: `RDD[(K,V)]`, где `K` - это страна, а `V` - высота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pairAirport = airportFinal.map(x => (x.isoCountry, x.elevationFt))\n",
    "pairAirport.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы не можем напрямую сравнивать два объекта `Option[T]`, то нам необходимо получить `T`. Будем считать, что аэропорты, где атрибут `elevationFt` принимает значение `None`, необходимо поместить в конец нашего списка. Для этого применим функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fixedElevation: RDD[(String, Int)] = pairAirport.map {\n",
    "    case (k, Some(v)) => (k, v)\n",
    "    case (k, None) => (k, Int.MinValue)\n",
    "}\n",
    "fixedElevation.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам необходимо применить функцию reduceByKey и получить нужный результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.max\n",
    "\n",
    "val result = fixedElevation.reduceByKey { (x, y) => Math.max(x,y) }.collect.sortBy( x => -x._2 )\n",
    "result.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- RDD API - это низкоуровневый API, который позволяет применять любые функции к распределенным данным\n",
    "- При использовании RDD API обработка всех исключительных ситуаций лежит на плечах разработчика\n",
    "\n",
    "После завершения работы не забывайте останавливать `SparkSession`, чтобы освободить ресурсы кластера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
