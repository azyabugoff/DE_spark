{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes III\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Обзор источников данных\n",
    "+ Текстовые форматы txt, csv, json\n",
    "+ Parquet и ORC\n",
    "+ Elastic\n",
    "+ Cassandra\n",
    "+ PostgreSQL\n",
    "\n",
    "## Обзор источников данных\n",
    "Spark - это платформа для **обработки** распределенных данных. Она не отвечает за хранение данных и не завязана на какую-либо БД или формат хранения, что позволяет разработать коннектор для работы с любым источником. Часть распространенных источников доступна \"из коробки\", часть - в виде сторонних библиотек. \n",
    "\n",
    "На текущий момент Spark DF API позволяет работать (читать и писать) с большим набором источников:\n",
    "+ Текстовые файлы:\n",
    "  - [json](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n",
    "  - text\n",
    "  - [csv](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)\n",
    "+ Бинарные файлы:\n",
    "  - [orc](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)\n",
    "  - [parquet](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)\n",
    "  - [delta](https://docs.delta.io/latest/quick-start.html)\n",
    "+ Базы данных\n",
    "  - [elastic](https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html#spark-sql)\n",
    "  - [cassandra](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md)\n",
    "  - [jdbc](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "  - [redis](https://github.com/RedisLabs/spark-redis/blob/master/doc/dataframe.md)\n",
    "  - [mongo](https://docs.mongodb.com/spark-connector/master/scala-api/)\n",
    "+ Стриминг системы\n",
    "  - [kafka](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "\n",
    "Для текстовых файлов поддерживаются различные кодеки сжатия (например `lzo`, `snappy`, `gzip`)\n",
    "\n",
    "### Добавление поддержки\n",
    "Чтобы добавить поддержку источника в проект, необходимо:\n",
    "+ найти нужный пакет на https://mvnrepository.com\n",
    " - выбрать актуальную версию для `Scala 2.11`\n",
    " - скачать jar или скопировать команду для нужной системы сборки \n",
    "+ добавить зависимость в `libraryDependencies` в файле `build.sbt`  \n",
    "```libraryDependencies += \"org.elasticsearch\" %% \"elasticsearch-spark-20\" % \"7.7.0\"```\n",
    "+ добавить зависимость в приложение одним из способов:\n",
    "  - добавить зависимость в **spark-submit**:  \n",
    "  ```spark-submit --packages org.elasticsearch:elasticsearch-spark-20_2.11:7.7.0```\n",
    "  - добавить jar файл в **spark-submit**:  \n",
    "  ```spark-submit --jars /path/to/elasticsearch-spark-20_2.11-7.7.0.jar```\n",
    "  - добавить зависимость в **spark-defaults.conf**:  \n",
    "  ```spark.jars.packages org.elasticsearch:elasticsearch-spark-20_2.11:7.7.0```\n",
    "  - добавить jar файл в **spark-defaults.conf**:  \n",
    "  ```spark.jars /path/to/elasticsearch-spark-20_2.11-7.7.0.jar```\n",
    "  - в коде через [`spark.sparkContext.addJar`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@addJar(path:String):Unit)\n",
    "  \n",
    "### Использование в коде\n",
    "Конфиги источника задаются одним из способов:\n",
    "- через **spark-submit**:  \n",
    "```spark-submit --conf spark.es.nodes=localhost:9200```\n",
    "- в **spark-defaults.conf**:  \n",
    "```spark.es.nodes localhost:9200```\n",
    "- в коде через **SparkSession**:\n",
    "  + ```spark.conf.set(\"spark.es.nodes\", \"localhost:9200\")```\n",
    "- в коде при чтении:  \n",
    "  + ```val df = spark.read.format(\"elastic\").option(\"es.nodes\", \"localhost:9200\")...```\n",
    "  + ```val df = spark.read.format(\"elastic\").options(Map(\"es.nodes\" -> \"localhost:9200\"))...```\n",
    "- в коде при записи:  \n",
    "  + ```df.write.format(\"elastic\").option(\"es.nodes\", \"localhost:9200\")...```\n",
    "  + ```df.write.format(\"elastic\").options(Map(\"es.nodes\" -> \"localhost:9200\"))...```\n",
    "  \n",
    "### Выводы:\n",
    "- Spark позволяет работать с болшим количеством источников\n",
    "- Поддержка источника всегда добавлеяется на уровне JVM (даже для pyspark) путем добавления в `java classpath` нужного класса\n",
    "- Добавить поддержку источника можно по-разному, однако в большинстве случаев следует избегать \"хардкода\"\n",
    "\n",
    "## Текстовые форматы\n",
    "\n",
    "Spark позволяет хранить данные в текстовом виде в форматах `text`, `json`, `csv`\n",
    "- `json` - JSON строки (не массив JSON документов, а именно раздельные строки, разделенные `\\n`)  \n",
    "- `csv` - плоские данные с разделителем  \n",
    "- `text` просто текстовые строки, вычитываются как DF с единственной колонкой `value: String`  \n",
    "\n",
    "### Преимущества:\n",
    "- простота интеграции\n",
    "- поддержка партиционирования и сжатия\n",
    "\n",
    "### Недостатки:\n",
    "- отсутствие оптимизаций \n",
    "- низкая скорость чтения сжатых данных\n",
    "- слабая типизация\n",
    "\n",
    "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "println(\"ls -al /tmp/datasets/airport-codes.csv\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем его в формате `csv`. Запись данных происходит в директорию, внутри которой будут файлы с данными. Это свойство является общим для всех файловых форматов в Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.mode(\"overwrite\").csv(\"/tmp/datasets/airports-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -al /tmp/datasets/airports-2.csv\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы попытаемся прочитать его с помощью `spark.read`, используя старый код, получим ошибку - в качестве схемы Spark взял одну из строк, содержащую данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airports-2.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поищем шапку в сырых данных - ее там не будет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.text(\"/tmp/datasets/airports-2.csv\").filter('value contains \"elevation_ft\").count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если прочитать с `header=false`, названия колонок будут автоматически сгенерированы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"false\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airports-2.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имея шапку в виде строки, мы можем создать схему самостоятельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val firstLine = \"head -n 1 /tmp/datasets/airport-codes.csv\".!!\n",
    "val schema = StructType(firstLine.split(\",\", -1).map(x => StructField(x, StringType)))\n",
    "val csvOptions = Map(\"header\" -> \"false\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.schema(schema).options(csvOptions).csv(\"/tmp/datasets/airports-2.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним данные в `csv` с включенной компрессией `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.mode(\"overwrite\").option(\"codec\", \"gzip\").csv(\"/tmp/datasets/airports-3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-3.csv\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные стали занимать меньше места, но у этого решения есть существенный минус - при чтении каждый сжатый файл превращается ровно в 1 партицию в DF. При работе с большими датасетами это означает:\n",
    "- если файлов мало и они большие, то воркерам может не хватить памяти для их чтения (тк один сжатый файл нельзя разбить на несколько партиций\n",
    "- если файлов много и они маленькие - мы получаем увеличенный расход памяти в heap HDFS NameNode (память расходуется пропорционально количеству файлов на HDFS из расчета 1 ГБ памяти на 1 000 000 файлов)\n",
    "\n",
    "Сохраним датасет в формате `json` с партиционирование по колонкам `iso_region` и `iso_country`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.mode(\"overwrite\").partitionBy(\"iso_region\", \"iso_country\").json(\"/tmp/datasets/airports-1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-1.json/iso_region=AD-04/iso_country=AD\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой формат хранения позволит использовать `partition pruning` и быстро фильтровать данные по колонкам `iso_region` и `iso_country`\n",
    "\n",
    "Теперь сохраним датасет в формат `text`. Для этого нам необходимо подгтовить DF, в котором будет единственная колонка `value: String`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .select('ident.alias(\"value\"))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"text\")\n",
    "    .save(\"/tmp/datasets/airports-1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-1.txt\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файловые форматы не имеют автоматической валидации данных при записи, поэтому достаточно легко ошибиться и записать данные в другом формате. Такая запись пройдет без ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .json(\"/tmp/datasets/airports-1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При попытке чтения данных с помощью `text` мы получим все данные, тк форма `json` сохраняет все в виде JSON строк. Однако, если прочитать данные с помощью `json`, часть данных будут помечены как невалидные и помещены в колонку `_corrupt_record`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airports = spark.read.json(\"/tmp/datasets/airports-1.txt\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим невалидные JSON строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Начиная со Spark 2.3 нельзя выбирать одну колонку _corrupt_record, поэтому мы добавим к выводу ident\n",
    "airports.na.drop(\"all\", Seq(\"_corrupt_record\")).select($\"_corrupt_record\", $\"ident\").show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Режимы записи\n",
    "Spark позволяет нам выбирать режим записи данных с помощью метода `mode()`. Данный метод принимает один из параметров:\n",
    "- `overwrite` - перезаписывает всю директорию целиком (или партицию, если используется партиционирование)\n",
    "- `append` - дописывает новые файлы к текущим\n",
    "- `ignore` - не выполняет запись (no op режим)\n",
    "- `error` или `errorifexists` - возвращает ошибку, если директория уже существует"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семплирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Форматы `csv` и `json` позволяют автоматически выводить схему из данных. При этом по-умолчанию Spark прочитает все данные и составит подходящую схему. Однако, если мы работаем с большим датасетом, это может занять продолжительное время. Решить это можно с помощью опции `samplingRatio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\", \"samplingRatio\" -> \"0.1\")\n",
    "    val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "    airports.printSchema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\", \"samplingRatio\" -> \"1.0\")\n",
    "    val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "    airports.printSchema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- Spark позволяет работать с текстовыми файлами `json`, `csv`, `text`\n",
    "- При чтении и записи поддерживаются кодеки сжатия данных, это создает дополнительные накладные расходы\n",
    "- При записи данных в текстовые форматы Spark **не выполняет** валидацию схемы и формата\n",
    "- При включенном выведении схемы из источника чтение из текстовых форматов происходит дольше\n",
    "\n",
    "## Orc и Parquet\n",
    "В отличие от обычных текстовых форматов, ORC и Parquet изначально спроектированы под распределенные системы хранения и обработки. Они являются колоночными - в них есть колонки и схема, как в таблицах БД и бинарными - прочитать обычным текстовым редактором их не получится. Форматы имеют похожие показатели производительности и архитектуру, но Parquet используется чаще\n",
    "\n",
    "### Преимущества\n",
    "- наличие схемы данных\n",
    "- блочная компрессия \n",
    "- для каждого блока для каждой колонки вычисляется max и min, что позволяет ускорять чтение\n",
    "\n",
    "### Недостатки:\n",
    "- нельзя дописывать/менять данные в существующих файлах\n",
    "- необходимо делать compaction\n",
    "\n",
    "Подробнее о Parquet:  \n",
    "[Фёдор Лаврентьев, Moscow Spark #5: Как класть Parquet](https://youtu.be/VHsvr10b63c?t=512)\n",
    "\n",
    "По аналогии с текстовыми форматами, при записи, Spark создает директорию и пишет туда все непустые партиции.  Обратите внимание на последовательность форматов записи - `snappy.parquet` вместо, скажем, `json.gz`. При использовании компрессии сам parquet файл не помещается в сжатый контейнер. Вместо этого, компрессии подлежат блоки с данными. Это полностью снимает ограничение, из-за которого чтение сжатых текстовых файлов происходит в 1 поток в 1 партицию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.mode(\"overwrite\").parquet(\"/tmp/datasets/airports-1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-1.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/tmp/datasets/airports-1.parquet\").printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с ORC/Parquet, часто возникает вопрос эволюции схемы - изменения структуры данных относительно первоначальных файлов. Создадим два DF с разными схемами и запишем их в одну директорию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Apple(size: Int, color: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(Apple(1, \"green\")).toDS.write.mode(\"append\").parquet(\"/tmp/datasets/apples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class PriceApple(size: Int, color: String, price: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(PriceApple(1, \"green\", 2.0)).toDS.write.mode(\"append\").parquet(\"/tmp/datasets/apples.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что файлы имеют разную схему, Spark корректно читает файлы, используя обобщенную схему:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.parquet(\"/tmp/datasets/apples.parquet\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, это работает только тогда, когда мы добавляем новые колонки к нашей схеме. Если мы запишем новый файл, изменив тип уже существующей колонки, мы получим ошибку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class AppleBase(size: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(AppleBase(3.0)).toDS.write.mode(\"append\").parquet(\"/tmp/datasets/apples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.parquet(\"/tmp/datasets/apples.parquet\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим все доступные опции для работы с Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dataframe --limit 20\n",
    "spark.sql(\"SET -v\").filter('key contains \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Tools\n",
    "Для диагностики и решения проблем, связанных с parquet, можно использовать утилиту `parquet-tools`\n",
    "\n",
    "Она позволяет:\n",
    "- получить схему файла\n",
    "- вывести содержимое файла в консоль\n",
    "- объединить несколько файлов в один\n",
    "\n",
    "https://github.com/apache/parquet-mr/tree/master/parquet-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение скорости обработки запросов\n",
    "\n",
    "Подготовим датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 to 100 foreach { x =>\n",
    "    airports.repartition(1).write.mode(\"append\").parquet(\"/tmp/datasets/a1.parquet\")\n",
    "    airports.repartition(1).write.mode(\"append\").json(\"/tmp/datasets/a1.json\")\n",
    "    airports.repartition(1).write.mode(\"append\").orc(\"/tmp/datasets/a1.orc\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "case class DatasetFormat[T](ds: Dataset[T], format: String)\n",
    "\n",
    "val datasets = \n",
    "    DatasetFormat(spark.read.orc(\"/tmp/datasets/a1.orc\"), \"orc\") ::\n",
    "    DatasetFormat(spark.read.parquet(\"/tmp/datasets/a1.parquet\"), \"parquet\") ::\n",
    "    DatasetFormat(spark.read.json(\"/tmp/datasets/a1.json\"), \"json\") ::\n",
    "    Nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним скорость работы фильтраци:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.foreach { x => \n",
    "    println(s\"Running ${x.format}\")\n",
    "    spark.time {\n",
    "        println(x.ds.filter($\"iso_country\" === \"RU\" and $\"elevation_ft\" > 300).count)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним скорость подсчета количества строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.foreach { x => \n",
    "    println(s\"Running ${x.format}\")\n",
    "    spark.time {\n",
    "        println(x.ds.count)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Форматы `orc` и `parquet` позволяют эффективно работать со структурированными данными\n",
    "- Производительность `orc` и `parquet` на порядок выше обычных текстовых файлов\n",
    "- Данные форматы поддерживают сжатие на блочном уровне, что позволяет избегать проблем с многопоточным чтением\n",
    "- Форматы поддерживают добавление новых колонок в схему, но не изменение текущих\n",
    "\n",
    "## Elastic\n",
    "Документориентированная распределенная база данных.\n",
    "\n",
    "### Преимущества:\n",
    "- Удобный графический интерфейс Kibana\n",
    "- Полнотекстовый поиск по любым колонкам\n",
    "- Встроенная поддержка timeseries\n",
    "- Поддержка вложенных структур\n",
    "- Возможность записи данных с произвольной схемой\n",
    "- Возможность перезаписывать данные по ключу документа\n",
    "\n",
    "### Недостатки:\n",
    "- Ассиметричная архитектура\n",
    "- Скорость записи ограничена самой медленным узлом\n",
    "- Большие накладные расходы CPU на индексирование\n",
    "- Ротация шардов не всегда проходит гладко\n",
    "\n",
    "https://www.elastic.co\n",
    "\n",
    "### Запуск в docker\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-dev-mode  \n",
    "https://www.elastic.co/guide/en/kibana/current/docker.html#_running_kibana_on_docker_for_development  \n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20  \n",
    "https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html  \n",
    "\n",
    "В Elastic есть несколько основных сущностей:\n",
    "\n",
    "**Index** - представляет собой \"таблицу с данными\", если проводить аналогию с реляционными БД. Данные в elastic обычно хранятся в виде индексов, разбитых на сутки (foo-2020-05-30, foo-2020-05-29 и т. д.). У каждого документа в индексе есть ключ `_id` и может быть метка времени, по которой в Kibana строятся визуализации\n",
    "\n",
    "**Template** - шаблон с параметрами, с которыми создается новый индекс. Пример шаблона представлен ниже:\n",
    "```shell\n",
    "PUT _template/airports\n",
    "{\n",
    "  \"index_patterns\": [\"airports-*\"],\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 1\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"_doc\": {\n",
    "      \"dynamic\": true,\n",
    "      \"_source\": {\n",
    "        \"enabled\": true\n",
    "      },\n",
    "      \"properties\": {\n",
    "        \"ts\": {\n",
    "          \"type\": \"date\",\n",
    "          \"format\": \"strict_date_optional_time||epoch_millis\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Shard** - индексы в elastic делятся ~~почкованием~~ на шарды. Это позволяет хранить индекс на нескольких узлах кластера\n",
    "\n",
    "**Index Pattern** - шаблон, применяемый к индексу на уровне Kibana. Позволяет настраивать форматирование и подсветку полей\n",
    "\n",
    "Перед тем, как начать писать в elastic с помощью Spark, нам необходимо создать шаблон, иначе индекс будет создан с параметрами по умолчанию и построить красивый pie chart в Kibana у нас не получится. Это можно сделать с помощью Dev Tools в Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val esOptions = \n",
    "    Map(\n",
    "        \"es.nodes\" -> \"localhost:9200\", \n",
    "        \"es.batch.write.refresh\" -> \"false\",\n",
    "        \"es.nodes.wan.only\" -> \"true\"   \n",
    "    )\n",
    "\n",
    "airports\n",
    "    .withColumn(\"ts\", current_timestamp())\n",
    "    .withColumn(\"date\", current_date())\n",
    "    .withColumn(\"foo\", lit(\"foo\"))\n",
    "    .write.format(\"es\").options(esOptions).save(\"airports-{date}/_doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val esDf = spark.read.format(\"es\").options(esOptions).load(\"airports-*\")\n",
    "esDf.printSchema\n",
    "esDf.show(1, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество партций в DF совпадает с общим числом шардов индексов, которые мы указали в `load()`. Поскольку у нас 1 индекс и 1 шард (см. шаблон), данный DF имеет 1 партицию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esDf.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К применяемым фильтрам применяется оптимизация `filter pushdown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esDf.filter('iso_region contains \"RU\").explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esDf\n",
    "    .filter(\n",
    "        'ts between (\n",
    "                    lit(\"2020-06-01 16:57:30.000\").cast(\"timestamp\"), \n",
    "                    lit(\"2020-06-01 16:59:30.000\").cast(\"timestamp\")\n",
    "        )\n",
    "    ).explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Elastic - удобное распределенное хранилище документов, не накладывающее строгих ограничений на схему документов\n",
    "- Elastic позволяет делать сложные запросы, включая полнотекстовые\n",
    "- При работе с elastic, Spark часто использует `filter pushdown`\n",
    "- Spark отлично подходит для того, чтобы писать в elastic. Однако чтение работает не очень быстро.\n",
    "\n",
    "## Cassandra\n",
    "Распределенная табличная база данных\n",
    "\n",
    "### Преимущества\n",
    "- Высокая доступность данных\n",
    "- Мжожно строить гео-кластера\n",
    "- Высокая скорость записи и чтения\n",
    "- Скорость ограничена самым быстрым узлом\n",
    "- Линейная масштабируемость\n",
    "- Возможность хранить БОЛЬШИЕ объемы данных\n",
    "- Возможность быстро получать строку по ключу на любом объеме данных\n",
    "\n",
    "### Недостатки\n",
    "- слабая согласованность (eventual)\n",
    "- Бедный SQL (в кассандре он называется CQL)\n",
    "- Отсутствие транзакций (не совсем)\n",
    "\n",
    "https://cassandra.apache.org\n",
    "\n",
    "Cassandra имеет симметричную архитектуру. Каждый узел отвечает за хранение данных, обработку запросов и состояние кластера. \n",
    "\n",
    "Расположение данных определяется значением хеш функции от Partition key.\n",
    "\n",
    "Высокая доступность данных обеспечивается за счет репликации.\n",
    "\n",
    "![Cassandra Architecture](https://cassandra.apache.org/doc/latest/_images/ring.svg)\n",
    "Источник: https://cassandra.apache.org/doc/latest/architecture/dynamo.html#dataset-partitioning-consistent-hashing\n",
    "\n",
    "### Запуск в docker\n",
    "\n",
    "Запуск инстанса:\n",
    "```shell\n",
    "docker run --rm --name cass -p 9042:9042 -e CASSANDRA_BROADCAST_ADDRESS=127.0.0.1 cassandra:latest\n",
    "```\n",
    "\n",
    "Подключение к cassandra:\n",
    "```shell\n",
    "docker run -it --rm cassandra:latest cqlsh host.docker.internal\n",
    "```\n",
    "\n",
    "В Cassandra есть:\n",
    "- `keyspace` - аналог database - логическое объединение таблиц. На уровне keyspace устанавливается фактор репликации\n",
    "- `table` - таблицы, как в обычной БД\n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector\n",
    "https://github.com/datastax/spark-cassandra-connector#documentation\n",
    "\n",
    "Для того, чтобы записать данные в cassandra, нам необходимо создать keyspace, используя утилиту `cqlsh`:\n",
    "```shell\n",
    "CREATE KEYSPACE IF NOT EXISTS airports \n",
    "WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3};\n",
    "```\n",
    "\n",
    "Теперь нужно создать таблицу со схемой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val typesMap = Map(\"string\" -> \"text\", \"int\" -> \"int\")\n",
    "\n",
    "val primaryKey = \"ident\"\n",
    "\n",
    "val ddlColumns = airports.schema.fields.map { x =>\n",
    "    if(x.name == primaryKey) {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)} PRIMARY KEY\"\n",
    "    }\n",
    "    else {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)}\"\n",
    "    }\n",
    "}.mkString(\",\")\n",
    "\n",
    "val ddlQuery = s\"CREATE TABLE IF NOT EXISTS airports.codes ($ddlColumns);\"\n",
    "\n",
    "println(ddlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим параметры подключения к БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.cassandra._\n",
    "\n",
    "spark.conf.set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "spark.conf.set(\"spark.cassandra.output.consistency.level\", \"ANY\")\n",
    "spark.conf.set(\"spark.cassandra.input.consistency.level\", \"ONE\")\n",
    "\n",
    "val tableOpts = Map(\"table\" -> \"codes\",\"keyspace\" -> \"airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем записать датасет в БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .write\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .mode(\"append\")\n",
    "    .options(tableOpts)\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark\n",
    "  .read\n",
    "  .format(\"org.apache.spark.sql.cassandra\")\n",
    "  .options(tableOpts)\n",
    "  .load()\n",
    "\n",
    "df.show(1, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорость чтения ОЧЕНЬ сильно зависит от структуры таблицы и запроса. Если мы сделаем запрос по колонке `ident`, которая является ключом, то будет применена оптимизация `filter pushdown` и запрос отработает очень быстро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('ident === \"22WV\").explain(true)\n",
    "\n",
    "spark.time { \n",
    "    df.filter('ident === \"22WV\").show(1, 200, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же мы сделаем запрос по другой колонке, то он не будет так же эффективен, хотя `filter pushdown` тоже отработает. Это происходит из-за того, что зная ключ, Cassandra знает, на каком хосте и где находятся данные. Когда мы фильтруем по колонке, которая не является ключом, БД приходится искать эти данные на всем кластере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('iso_region === \"RU\").explain(true)\n",
    "\n",
    "spark.time {\n",
    "    df.filter('iso_region === \"RU\").show(1, 200, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если сделать запрос более сложным, то `filter pushdown` не отработает и Spark прочитает всю таблицу целиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "df.filter(lower('iso_region) === \"ru\").explain(true)\n",
    "\n",
    "spark.time {\n",
    "    df.filter(lower('iso_region) === \"ru\").show(1, 200, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо понимать, что структура данной таблицы не отражает реального паттерна работой с данной базой - редко ключом является только одна колонка. В реальности ключ является составным и состоит из нескольких колонок, что позволяет делать более сложные запросы, которые будут обрабатываться также быстро, как первый запрос с фильтрацией по колонке \"ident\"\n",
    "\n",
    "### Выводы\n",
    "- cassandra - одна из немногих БД, которая способна эффективно хранить большие объемы данных\n",
    "- в cassandra структура таблицы формируется, исходя из запросов, которые будут выполняться, а не наоборот\n",
    "- в данной БД данные обычно хранятся в денормализованном виде (если утрировать - то по таблице на каждый запрос)\n",
    "- Spark отлично подходит для чтения и записи данных в cassandra, но ее придется настроить под данный профиль нагрузки\n",
    "\n",
    "## PostgreSQL\n",
    "Классическая РБД\n",
    "\n",
    "### Преимущества:\n",
    "- это PostgreSQL\n",
    "\n",
    "### Недостатки\n",
    "- нет их\n",
    "\n",
    "### Запуск в docker\n",
    "```shell\n",
    "docker run --rm -p 5432:5432 --name test_postgre -e POSTGRES_PASSWORD=12345 postgres:latest\n",
    "```\n",
    "\n",
    "Подключение с помощью psql:\n",
    "```shell\n",
    "docker run -it --rm postgres psql -h host.docker.internal -U postgres\n",
    "```\n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/org.postgresql/postgresql\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "\n",
    "Для работы с БД создадим database:\n",
    "```shell\n",
    "CREATE DATABASE airports\n",
    "```\n",
    "Подготовим DDL для создания таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val typesMap = Map(\"string\" -> \"VARCHAR (100)\", \"int\" -> \"INTEGER\")\n",
    "\n",
    "val primaryKey = \"ident\"\n",
    "\n",
    "val ddlColumns = airports.schema.fields.map { x =>\n",
    "    if(x.name == primaryKey) {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)} PRIMARY KEY\"\n",
    "    }\n",
    "    else {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)}\"\n",
    "    }\n",
    "}.mkString(\",\")\n",
    "\n",
    "val ddlQuery = s\"CREATE TABLE IF NOT EXISTS codes ($ddlColumns);\"\n",
    "\n",
    "val jdbcUrl = \"jdbc:postgresql://localhost/airports?user=postgres&password=12345\"\n",
    "\n",
    "println(ddlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем данные в БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\", \"codes\").mode(\"append\").save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbcUrl)\n",
    "    .option(\"dbtable\", \"codes\")\n",
    "    .load()\n",
    "\n",
    "df.printSchema\n",
    "df.show(2, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании параметром по умолчанию мы получаем всего 1 партицию в DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправить это можно, используя параметры `partitionColumn`, `lowerBound`, `upperBound`, `numPartitions`. Для этого нам понадобится добавиь новую колонку в нашу таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ddlColumns = airports.schema.fields.map { x =>\n",
    "    if(x.name == primaryKey) {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)} PRIMARY KEY\"\n",
    "    }\n",
    "    else {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)}\"\n",
    "    }\n",
    "} :+ \"id INTEGER\" mkString(\",\")\n",
    "\n",
    "val ddlQuery = s\"CREATE TABLE IF NOT EXISTS codes_x ($ddlColumns);\"\n",
    "\n",
    "println(ddlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перезапишем данные в новую таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "airports\n",
    "    .withColumn(\"id\", round(rand() * 10000).cast(\"int\"))\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbcUrl)\n",
    "    .option(\"dbtable\", \"codes_x\")\n",
    "    .mode(\"append\").save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем таблицу, установив дополнительные параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbcUrl)\n",
    "    .option(\"dbtable\", \"codes_x\")\n",
    "    .option(\"partitionColumn\", \"id\")\n",
    "    .option(\"lowerBound\", \"0\")\n",
    "    .option(\"upperBound\", \"10000\")\n",
    "    .option(\"numPartitions\", \"200\")\n",
    "    .load()\n",
    "\n",
    "df.printSchema\n",
    "df.show(2, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, сколько партиций получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим распределение данных по партициям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "df.groupBy(spark_partition_id()).count().show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет работать с PostgreSQL через JDBC коннектор\n",
    "- При использовании `jdbc` настройка партиционирования задается вручную"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
