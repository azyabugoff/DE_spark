{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes II\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Планы выполнения задач\n",
    "+ Оптимизация соединений и группировок\n",
    "+ Управление схемой данных\n",
    "+ Оптимизатор запросов Catalyst\n",
    "\n",
    "## Планы выполнения задач\n",
    "\n",
    "Любой `job` в Spark SQL имеет под собой план выполнения, кототорый генерируется на основе написанно запроса. План запроса содержит операторы, которые затем превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы, например:\n",
    "+ убрать лишние shuffle\n",
    "+ убедиться, чтот тот или иной оператор будет выполнен на уровне источника, а не внутри Spark\n",
    "+ понять, как будет выполнен `join`\n",
    "\n",
    "Планы выполнения доступны в двух видах:\n",
    "+ метод `explain()` у DF\n",
    "+ на вкладке SQL в Spark UI\n",
    "\n",
    "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод `explain`, чтобы посмотреть план запроса. Наиболее интересным является физический план, т.к. он отражает фактически алгоритм обработки данных. В данном случае в плане присутствует единственный оператор `FileScan csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.explain(extended = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если остальные планы не нужны, можно показать только физический:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "airports.queryExecution.executedPlan.treeString\n",
    "\n",
    "def printPhysicalPlan[_](ds: Dataset[_]): Unit = {\n",
    "    println(ds.queryExecution.executedPlan.treeString)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также есть возмжность получить эту информацию в виде JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.queryExecution.executedPlan.toJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним `filter` и проверим план выполнения. Читать план нужно снизу вверх. В плане появился новый оператор `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printPhysicalPlan(airports.filter('type === \"small_airport\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним агрегацию и проверим план выполнения. В нем появляется три оператора: 2 `HashAggregate` и `Exchange hashpartitioning`.\n",
    "\n",
    "Первый `HashAggregate` содержит функцию `partial_count(1)`. Это означает, что внутри каждого воркера произойдет подсчет строк по каждому ключу. Затем происходит `shuffle` по ключу агрегата, после которого выполняется еще один `HashAggregate` с функцией `count(1)`. Использование двух `HashAggregate` позволяет сократить количество передаваемых данных по сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printPhysicalPlan(airports.filter('type === \"small_airport\").groupBy('iso_country).count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости мы можем почитать ~~перед сном~~ сгенерированный ~~теплый ламповый~~ java код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.execution.command.ExplainCommand\n",
    "\n",
    "val grouped = airports.filter('type === \"small_airport\").groupBy('iso_country).count\n",
    "\n",
    "\n",
    "def printCodeGen[_](ds: Dataset[_]): Unit = {\n",
    "    val logicalPlan = ds.queryExecution.logical\n",
    "    val codeGen = ExplainCommand(logicalPlan, extended = true, codegen = true)\n",
    "    spark.sessionState.executePlan(codeGen).executedPlan.executeCollect().foreach {\n",
    "      r => println(r.getString(0))\n",
    "    }\n",
    "}\n",
    "\n",
    "printCodeGen(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://cs5.pikabu.ru/post_img/big/2015/12/11/7/1449830295198229367.jpg\">\n",
    "\n",
    "### Выводы:\n",
    "+ Spark составляет физический план выполнения запроса на основании написанного вами кода\n",
    "+ Изучив план запроса, можно понять, какие операторы будут применены в ходе обработки ваших данных\n",
    "+ План выполнения запроса - один из основных инструментов оптимизации запроса\n",
    "\n",
    "## Оптимизация соединений и группировок\n",
    "При выполнении `join` двух DF важно следовать рекомендациям:\n",
    "+ фильтровать данные до join'а\n",
    "+ использовать equ join \n",
    "+ если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так\n",
    "+ всеми силами избегать cross-join'ов\n",
    "+ если правый DF помещается в памяти worker'а, использовать broadcast()\n",
    "\n",
    "### Виды соединений\n",
    "+ **BroadcastHashJoin**\n",
    "  - equ join\n",
    "  - broadcast\n",
    "+ **SortMergeJoin**\n",
    "  - equ join\n",
    "  - sortable keys\n",
    "+ **BroadcastNestedLoopJoin**\n",
    "  - non-equ join\n",
    "  - using broadcast\n",
    "+ **CartesianProduct**\n",
    "  - non-equ join\n",
    "  \n",
    "[Optimizing Apache Spark SQL Joins: Spark Summit East talk by Vida Ha](https://youtu.be/fp53QhSfQcI)\n",
    "\n",
    "Подготовим два датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val left = airports.select('type, 'ident, 'iso_country)\n",
    "val right = airports.groupBy('type).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastHashJoin\n",
    "+ работает, когда условие - равенство одного или нескольких ключей\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ составляет hash map из правого датасета, где ключ - кортеж из колонок в условии соединения\n",
    "+ итерируется по левому датасета внутри каждой партиции и проверяет наличие ключей в HashMap\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.broadcast\n",
    "val result = left.join(broadcast(right), Seq(\"type\"), \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortMergeJoin\n",
    "+ работает, когда ключи соединения в обоих датасета являются сортируемыми\n",
    "+ репартиционирует оба датасета в 200 партиций по ключу (ключам) соединения\n",
    "+ сортирует партиции каждого из датасетов по ключу (ключам) соединения\n",
    "+ Используя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "val result = left.join(right, Seq(\"type\"), \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastNestedLoopJoin\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ проходится вложенным циклом по каждой партиции левого датасета и копией правого датасета и проверяет условие\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{ expr, udf, col }\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "// Не смотря на то, что UDF сравнивает два ключа, Spark ничего про нее не знает\n",
    "// и не может применить BroadcastHashJoin или SortMergeJoin\n",
    "val compare_udf = udf { (leftVal: String, rightVal: String) => leftVal == rightVal }\n",
    "\n",
    "val joinExpr = compare_udf(col(\"left.type\"), col(\"right.type\"))\n",
    "\n",
    "val result = left.as(\"left\").join(broadcast(right).as(\"right\"), joinExpr, \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartesianProduct\n",
    "+ Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения\n",
    "+ на выходе создает N*M партиций\n",
    "+ работает медленнее остальных и часто приводит к ООМ воркеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{ expr, udf, col }\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "// Не смотря на то, что UDF сравнивает два ключа, Spark ничего про нее не знает\n",
    "// и не может применить BroadcastHashJoin или SortMergeJoin\n",
    "val compare_udf = udf { (leftVal: String, rightVal: String) => leftVal == rightVal }\n",
    "\n",
    "val joinExpr = compare_udf(col(\"left.type\"), col(\"right.type\"))\n",
    "\n",
    "val result = left.as(\"left\").join(right.as(\"right\"), joinExpr, \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)\n",
    "println(\n",
    "    s\"\"\"Partition summary: \n",
    "    left=${left.rdd.getNumPartitions}, \n",
    "    right=${right.rdd.getNumPartitions}, \n",
    "    result=${result.rdd.getNumPartitions}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Снижение количества shuffle\n",
    "В ряде случаев можно уйти от лишних `shuffle` операций при выполнении соединения. Для этого оба DF должны иметь одинаковое партиционирование - одинаковое количество партиций и ключ партиционирования, совпадающий с ключом соединения.\n",
    "\n",
    "Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val left = airports\n",
    "    val right = airports.groupBy('type).count\n",
    "\n",
    "    val joined = left.join(right, Seq(\"type\"))\n",
    "\n",
    "    joined.count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val airportsRep = airports.repartition(200, col(\"type\"))\n",
    "    val left = airportsRep\n",
    "    val right = airportsRep.groupBy('type).count\n",
    "\n",
    "    val joined = left.join(right, Seq(\"type\"))\n",
    "\n",
    "    joined.count\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ В Spark используются 4 вида соединений: `BroadcastHashJoin`, `SortMergeJoin`, `BroadcastNestedLoopJoin`, `CartesianProduct`\n",
    "+ Выбор алгоритма основывается на условии соединения и размере датасетов\n",
    "+ `CartesianProduct` обладает самой низкой вычислительной эффективностью и его по возможности стоит избегать\n",
    "\n",
    "## Управление схемой данных\n",
    "В DF API каждая колонка имеет свой тип. Он может быть:\n",
    "+ скаляром - `StringType`, `IntegerType` и т. д.\n",
    "+ массивом - `ArrayType(T)`\n",
    "+ словарем `MapType(K, V)`\n",
    "+ структурой - `StructType()`\n",
    "\n",
    "DF целиком также имеет схему, описанную с помощью класса `StructType`\n",
    "\n",
    "Посмотреть список колонок можно с помощью атрибута `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема DF доступна через атрибут `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schema: StructType = airports.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply()` метод возвращает поле структуры по имени, как в словаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val field: StructField = schema(\"ident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StructField` обладает атрибутами `name` и `dataType`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val name: String = field.name\n",
    "\n",
    "val fieldType: DataType = field.dataType\n",
    "\n",
    "fieldType match {\n",
    "    case f: StringType => println(\"This is string\")\n",
    "    case _ => println(\"This is not string!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `simpleString` можно использовать, чтобы получить DDL схемы в виде строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldType.simpleString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportSchema = schema.simpleString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема может быть создана из `case class`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "case class Airport(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevation_ft: Int,\n",
    "    continent: String,\n",
    "    iso_country: String,\n",
    "    iso_region: String,\n",
    "    municipality: String,\n",
    "    gps_code: String,\n",
    "    iata_code: String,\n",
    "    local_code: String,\n",
    "    coordinates: String\n",
    ")\n",
    "\n",
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "val schemaFromClass = ScalaReflection.schemaFor[Airport].dataType.asInstanceOf[StructType]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема может быть использована:\n",
    "+ при чтении источника\n",
    "+ при работе с JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"false\")\n",
    "val airports = spark.read.options(csvOptions).schema(schemaFromClass).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val parseJson = from_json(col(\"value\"), schemaFromClass).alias(\"s\")\n",
    "\n",
    "val jsoned = airports.toJSON\n",
    "\n",
    "val withColumns = jsoned.select(parseJson).select(col(\"s.*\"))\n",
    "\n",
    "withColumns.show(1, 200, true)\n",
    "withColumns.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема может быть создана вручную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val someSchema = \n",
    "    StructType(\n",
    "        List(\n",
    "            StructField(\"foo\", StringType),\n",
    "            StructField(\"bar\", StringType),\n",
    "            StructField(\n",
    "                        \"boo\", \n",
    "                        StructType(\n",
    "                            List(\n",
    "                                StructField(\"x\", IntegerType),\n",
    "                                StructField(\"y\", BooleanType)\n",
    "                                )\n",
    "                            )\n",
    "                       )\n",
    "        \n",
    "        )\n",
    "    )\n",
    "\n",
    "someSchema.printTreeString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема также может быть получена из JSON строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jsoned = airports.toJSON\n",
    "\n",
    "val firstLine = jsoned.head\n",
    "\n",
    "spark.range(1).select(schema_of_json(lit(firstLine))).head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы изменить тип колонки, следует использовать метод `cast`. Данная операция может как возвращать `null`, так и бросать исключение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.select('elevation_ft.cast(\"string\")).printSchema\n",
    "airports.select('elevation_ft.cast(\"string\")).show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.select('type.cast(\"float\")).printSchema\n",
    "airports.select('type.cast(\"float\")).show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Spark использует схемы для описания типов колонок, схемы всего DF, чтения источников и для работы с JSON\n",
    "+ Схема представляет собой инстанс класса `StructType`\n",
    "+ Колонки в Spark могут иметь любой тип. При этом вложенность словарей, массивов и структур не ограничена\n",
    "\n",
    "## Оптимизатор запросов Catalyst\n",
    "Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:\n",
    " + Column projection\n",
    " + Partition pruning\n",
    " + Predicate pushdown\n",
    " + Constant folding\n",
    " \n",
    " Подготовим датасет для демонстрации работы Catalyst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .partitionBy(\"iso_country\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/tmp/datasets/airports.parquet\")\n",
    "\n",
    "val airportPq = spark.read.parquet(\"/tmp/datasets/airports.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column projection\n",
    "Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val selected = airportPq.select('ident)\n",
    "    selected.cache\n",
    "    selected.count\n",
    "    selected.unpersist\n",
    "    printPhysicalPlan(selected)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val selected = airportPq\n",
    "    selected.cache\n",
    "    selected.count\n",
    "    selected.unpersist\n",
    "    printPhysicalPlan(selected)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition pruning\n",
    "Данный механизм позволяет избежать чтения ненужных партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val filtered = airportPq.filter('iso_country === \"RU\")\n",
    "    filtered.count\n",
    "    printPhysicalPlan(filtered)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicate pushdown\n",
    "Данный механизм позволяет \"протолкнуть\" условия фильтрации данных на уровень datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    val filtered = airportPq.filter('iso_region === \"RU\")\n",
    "    filtered.count\n",
    "    printPhysicalPlan(filtered)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify casts\n",
    "Данный механизм убирает ненужные `cast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = spark.range(0,10).select('id.cast(\"long\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = spark.range(0,10).select('id.cast(\"int\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant folding\n",
    "Данный механизм сокращает количество констант, используемых в физическом плане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = spark.range(0,10).select((lit(3) >  lit(0)).alias(\"foo\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = spark.range(0,10).select(('id >  0).alias(\"foo\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine filters\n",
    "Данный механизм объединяет фильтры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = spark.range(0,10).filter('id > 0).filter('id !== 5).filter('id < 10)\n",
    "printPhysicalPlan(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
