{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Structured Streaming II\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Что такое stateful streaming\n",
    "+ Удаление дубликатов\n",
    "+ Агрегаты\n",
    "+ Соединения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое stateful streaming\n",
    "\n",
    "**stateful streaming** - это вид поточной обработки данных, при которой при обработке батча с данными используются данные из предыдущих батчей\n",
    "\n",
    "Все операции с использованием select, filter, withColumn (кроме операций с плавающими окнами) являются stateless. На практике это означает:\n",
    "- стрим не выполняет операций, требующих работы с данными из разных батчей\n",
    "- после обработки батча стрим \"забывает\" про него\n",
    "- высокую пропускную способность\n",
    "- небольшое количество файлов и общий объем чекпоинта\n",
    "- возможность вносить существенные правки в код стрима без пересоздания чекпоинта\n",
    "\n",
    "Если при обработке стрима используются такие методы, как `join()`, `groupBy()`, `dropDuplicates()` или функции над плавающими окнами, то:\n",
    "- в стриме должна быть колонка с временной меткой, на основе которой можно определить `watermark`\n",
    "- стрим будет работать медленней, чем вы ожидаете\n",
    "- в чекпоинте будет МНОГО файлов\n",
    "- при внесении изменений в код стрима с большой вероятностью придется пересоздавать чекпоинт\n",
    "\n",
    "Подготовим функции для управления стримами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSink(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"/tmp/chk/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def airportsDf() = {\n",
    "    val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "    spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "}\n",
    "\n",
    "def randomIdent() = {\n",
    "    \n",
    "    val idents = airportsDf().select('ident).limit(20).distinct.as[String].collect\n",
    "\n",
    "    val columnArray = idents.map( x => lit(x) )\n",
    "    val sparkArray = array(columnArray:_*)\n",
    "    val shuffledArray = shuffle(sparkArray)\n",
    "\n",
    "    shuffledArray(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление дубликатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark позволяет удалять дубликаты данных в стриме. Это можно сделать двумя способами:\n",
    "- без использования `watermark`\n",
    "- с использованием `watermark`\n",
    "\n",
    "### Без использования watermark\n",
    "- Хеш каждого элемента будет сохраняться в чекпоинте\n",
    "- В стриме полностью исключаются дубликаты\n",
    "- Со временем начнется деградация стрима"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"rm -rf /tmp/chk\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sdfWithDuplicates = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state1\", sdfWithDuplicates).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sdfWithoutDuplicates = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .dropDuplicates(Seq(\"ident\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state2\", sdfWithoutDuplicates).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С использованием watermark\n",
    "- Хеш старых событий удаляется из чекпоинта\n",
    "- Появление дубликатов возможно, если они приходят c задержкой N > watermark\n",
    "- Стрим не деградирует со временем\n",
    "- Колонка, по которой делается `watermark`, должна быть включена в `dropDuplicates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sdfWithoutDuplicates = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    .dropDuplicates(Seq(\"ident\", \"timestamp\"))\n",
    "\n",
    "createConsoleSink(\"state3\", sdfWithoutDuplicates).start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет удалять дубликаты из стрима\n",
    "- Для стабильной работы требуется использовать `watermark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Агрегаты\n",
    "При построении агрегатов на стриме важно задать правильный `outputMode`, который может иметь три значения:\n",
    "- `append`\n",
    "- `update`\n",
    "- `complete`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"rm -rf /tmp/chk\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSink(chkName: String, mode: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .outputMode(mode)\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"/tmp/chk/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val grouped = \n",
    "    sdfWithDuplicates.groupBy('ident).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state3\", \"complete\", grouped).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state4\", \"update\", grouped).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Агрегаты на плавающих окнах\n",
    "Плавающее окно позволяет сгруппировать события в окна определенного размера (по времени). При этом, поскольку каждое событие может находится одновременно в нескольких окнах, то общий размер агрегата существенно увеличивается\n",
    "\n",
    "Окно задается при создании агрегата с помощью функции `window` внутри `groupBy`. В параметрах указывается длина окна и расстояние между двумя точкой начала текущего и следующего окна.\n",
    "\n",
    "<img align=\"center\" width=\"1000\" height=\"1000\" src=\"https://spark.apache.org/docs/latest/img/structured-streaming-window.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"rm -rf /tmp/chk\".!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим стрим с использованием `window` и `watermark` в `update` режиме. В данном случае watermark позволяет игнорировать события, которые приходят с запозданием (`latency` > `max_event_timestamp` + `watermark_value`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val oldData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", lit(\"OLD DATA\"))\n",
    "    .withColumn(\"timestamp\", date_sub('timestamp, 1))\n",
    "\n",
    "createConsoleSink(\"state10\", \"append\", oldData).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val newData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "\n",
    "val oldData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", lit(\"OLD DATA\"))\n",
    "    .withColumn(\"timestamp\", date_sub('timestamp, 1))\n",
    "\n",
    "val uData = newData\n",
    "    .union(oldData)\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    .groupBy(window($\"timestamp\", \"10 minutes\"), 'ident)\n",
    "    .count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "createConsoleSink(\"state5\", \"update\", uData).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим стрим с использованием `window` и `watermark` в `append` режиме. В `append` режиме в синк будут записаны только завершенные окна с данными в момент `window_right_bound` + `watermark_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val newData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "\n",
    "val uData = newData\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\")\n",
    "    .groupBy(window($\"timestamp\", \"1 minutes\", \"1 minutes\"))\n",
    "    .count\n",
    "\n",
    "createConsoleSink(\"state6\", \"append\", uData).start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- Spark позволяет строить агрегаты на SDF в разных режимах\n",
    "- Вотермарки поддерживаются при использовании `append` и `update` режимов\n",
    "- Плавающее окно имеет два параметра - размер окна и сдвиг текущего окна относительно следующего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединения\n",
    "Spark позволяет делать:\n",
    "- stream - static join\n",
    "  + inner\n",
    "  + left outer\n",
    "  + left anti\n",
    "- stream - stream join\n",
    "  + inner\n",
    "  + left outer\n",
    "  + right outer\n",
    "  \n",
    "### Stream-Static join\n",
    "Может использоваться для:\n",
    "- обогащения стрима фактами (left outer)\n",
    "- фильтрации по blacklist (left anti)\n",
    "- фильтрации по whitelist (inner)\n",
    "\n",
    "#### Left outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val identStream = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val right = airportsDf()\n",
    "\n",
    "val result = identStream.join(right, Seq(\"ident\"), \"left\").select('ident, 'name, 'elevation_ft, 'iso_country)\n",
    "\n",
    "result.printSchema\n",
    "\n",
    "result.explain(true)\n",
    "\n",
    "createConsoleSink(\"state7\", result).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val right = Vector(\"00FA\", \"00IG\", \"00FD\").toDF.withColumnRenamed(\"value\", \"ident\")\n",
    "val result = identStream.join(right, Seq(\"ident\"), \"inner\")\n",
    "\n",
    "createConsoleSink(\"state8\", result).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val right = Vector(\"00FA\", \"00IG\", \"00FD\").toDF.withColumnRenamed(\"value\", \"ident\")\n",
    "val result = identStream.join(right, Seq(\"ident\"), \"left_anti\")\n",
    "\n",
    "createConsoleSink(\"state9\", result).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream-Stream join\n",
    "Для соединения двух стримов нам необходимо добавить к условию соединения равенство двух окон или сравнение двух временных меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"rm -rf /tmp/chk\".!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val left = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"left\", lit(\"left\"))\n",
    "    .withWatermark(\"timestamp\", \"2 hours\")\n",
    "    .withColumn(\"window\", window('timestamp, \"1 minute\")).as(\"left\")\n",
    "\n",
    "val right = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"right\", lit(\"right\"))\n",
    "    .withWatermark(\"timestamp\", \"3 hours\")\n",
    "    .withColumn(\"window\", window('timestamp, \"1 minute\")).as(\"right\")\n",
    "\n",
    "val joinExpr = expr(\"\"\"left.value = right.value and left.window = right.window\"\"\")\n",
    "\n",
    "val joined = left.join(right, joinExpr, \"inner\").select($\"left.window\", $\"left.value\", $\"left\", $\"right\")\n",
    "\n",
    "// joined.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state10\", joined).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"rm -rf /tmp/chk\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val left = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"left\", lit(\"left\"))\n",
    "    .withWatermark(\"timestamp\", \"2 hours\").as(\"left\")\n",
    "\n",
    "val right = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"right\", lit(\"right\"))\n",
    "    .withWatermark(\"timestamp\", \"3 hours\").as(\"right\")\n",
    "\n",
    "val joinExpr = expr(\"\"\"left.value = right.value and left.timestamp <= right.timestamp + INTERVAL 1 hour \"\"\")\n",
    "\n",
    "val joined = left.join(right, joinExpr, \"inner\").select($\"left.value\", $\"left.timestamp\", $\"left\", $\"right\")\n",
    "\n",
    "joined.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state11\", joined).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет соединять SDF со статическим DF, используя разные виды соединений: left outer, inner, left anti\n",
    "- Допускается использование соединений двух стримов, для этого требуется использовать вотермарки и (опционально) плавающие окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
