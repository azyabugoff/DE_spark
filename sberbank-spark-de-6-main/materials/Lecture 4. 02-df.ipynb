{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes I\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Сравнение RDD API и DataFrame API \n",
    "+ Базовые функции\n",
    "+ Очистка данных\n",
    "+ Агрегаты\n",
    "+ Кеширование \n",
    "+ Репартиционирование\n",
    "+ Встроенные функции\n",
    "+ Пользовательские функции\n",
    "+ Соединения\n",
    "+ Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение RDD API и DataFrame API \n",
    "\n",
    "### Типы данных\n",
    "**RDD**: низкоуревная распределенная коллекция данных любого типа  \n",
    "**DF**: таблица со схемой, состоящей из колонок разных типов, описанных в `org.apache.spark.sql.types`  \n",
    "\n",
    "### Обработка данных\n",
    "**RDD**: сериализуемые функции  \n",
    "**DF**: кодогенерация SQL > Java код  \n",
    "\n",
    "### Функции и алгоритмы\n",
    "**RDD**: нет ограничений  \n",
    "**DF**: ограничен SQL операторами, функциями `org.apache.spark.sql.functions` и пользовательскими функциями  \n",
    "\n",
    "### Источники данных\n",
    "**RDD**: каждый источник имеет свое API  \n",
    "**DF**: единое API для всех источников \n",
    "\n",
    "### Производительность\n",
    "**RDD**: напрямую зависит от качества кода\n",
    "**DF**: встроенные механизмы оптимизации SQL запроса\n",
    "\n",
    "\n",
    "### Потоковая обработка данных\n",
    "**RDD**: устаревший DStreams  \n",
    "**DF**: активно развивающийся Structured Streaming\n",
    "\n",
    "\n",
    "### Выводы:\n",
    "+ На текущий момент RDD является низкоуровневым API, которое постепенно уходит \"под капот\" Apache Spark\n",
    "+ DF API представляет собой библиотеку для обработки данных с использованием SQL примитивов\n",
    "\n",
    "## Базовые функции\n",
    "\n",
    "Создать dataframe можно на основе:\n",
    "+ локальной коллекции\n",
    "+ файлов\n",
    "+ базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cityList = Vector(Moscow, Paris, Madrid, London, New York)\n",
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "val cityList: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")\n",
    "\n",
    "// метод toDF изначально отсутствует у Vector[T], он добавляется через import spark.implicits._\n",
    "val df: DataFrame = cityList.toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У любого DF есть схема:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть содержимое DF можно с помощью метода `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|  Moscow|\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно вывести содержимое в вертикальной ориентации - это удобно при большое количестве столбцов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " value | Moscow   \n",
      "-RECORD 1---------\n",
      " value | Paris    \n",
      "-RECORD 2---------\n",
      " value | Madrid   \n",
      "-RECORD 3---------\n",
      " value | London   \n",
      "-RECORD 4---------\n",
      " value | New York \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(numRows = 20, truncate = 100, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет количества элементов в DF с помощью `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтровать данные можно с помощью метода `filter`. В отличие от RDD, он принимает SQL выражение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(value#5) && (value#5 = Moscow))\n",
      "+- Scan ExistingRDD[value#5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo = (value = Moscow)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(value = Moscow)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val foo: Column = col(\"value\").===(\"Moscow\")\n",
    "\n",
    "df.localCheckpoint.filter(foo).show\n",
    "df.localCheckpoint.filter(foo).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter($\"value\" === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// sugar free & type safe\n",
    "// Три знака равно здесь используются, тк на самом деле это метод,\n",
    "// применяемый к колонке org.apache.spark.sql.Column\n",
    "\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "df.filter(col(\"value\") === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(value#5) && (value#5 = Moscow))\n",
      "+- Scan ExistingRDD[value#5]\n"
     ]
    }
   ],
   "source": [
    "// легко ошибиться и получить ошибку в рантайме\n",
    "\n",
    "df.localCheckpoint.filter(\"value = 'Moscow'\").show\n",
    "df.localCheckpoint.filter(\"value = 'Moscow'\").explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// промежуточный вариант между col и обычной строкой\n",
    "// expr также может использоваться для вызова SQL builtin функций, \n",
    "// отсутствующих в org.apache.spark.sql.functions\n",
    "\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "df.filter(expr(\"value = 'Moscow'\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить новую колонку можно с помощью метода `withColumn`. Необходимо помнить, что данный метод, как и другие, является трансформацией и не изменяет оригинальный DF, а создает новый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [value#5, upper(value#5) AS upperCity#89]\n",
      "+- Scan ExistingRDD[value#5]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.upper\n",
    "df.withColumn(\"upperCity\", upper('value)).show\n",
    "df.localCheckpoint.withColumn(\"upperCity\", upper('value)).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный результат получить, используя метод `select`. Данный метод может быть использован не только для выбора определенных колонок, но и для создания новых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [value#5, upper(value#5) AS upperCity#108]\n",
      "+- Scan ExistingRDD[value#5,upperCity#103]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withUpper = [value: string, upperCity: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string, upperCity: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withUpper = df.select('value, upper('value).alias(\"upperCity\"))\n",
    "withUpper.show\n",
    "df.select('value, upper('value).alias(\"upperCity\")).localCheckpoint.withColumn(\"upperCity\", upper('value)).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[value#5]\n"
     ]
    }
   ],
   "source": [
    "df.localCheckpoint.select(col(\"*\")).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myProjection = [value: string, foo: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string, foo: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myProjection = df.select(col(\"value\"), lit(\"foo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCols = List(value, foo, false, named_struct(NamePlaceholder(), value AS `woo`) AS `moo`)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(value, foo, false, named_struct(NamePlaceholder(), value AS `woo`) AS `moo`)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myCols = col(\"value\") :: lit(\"foo\") :: lit(false) :: struct(col(\"value\").alias(\"woo\")).alias(\"moo\") :: Nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- foo: string (nullable = false)\n",
      " |-- false: boolean (nullable = false)\n",
      " |-- moo: struct (nullable = false)\n",
      " |    |-- woo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(myCols:_*).printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если передать `col(\"*\")` в `select`, то вы получите DF со всеми колонками. Это полезно, когда вы не знаете список всех колонок (например вы получили его через API), но вам нужно их все выбрать и добавить новую колонку. Это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+---+\n",
      "|   value|upperCity|lowerCity|length|bar|\n",
      "+--------+---------+---------+------+---+\n",
      "|  Moscow|   MOSCOW|   moscow|     7|foo|\n",
      "|   Paris|    PARIS|    paris|     6|foo|\n",
      "|  Madrid|   MADRID|   madrid|     7|foo|\n",
      "|  London|   LONDON|   london|     7|foo|\n",
      "|New York| NEW YORK| new york|     9|foo|\n",
      "+--------+---------+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// методы name, as и alias часто являются взаимозаменяемыми\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "withUpper.select(\n",
    "    col(\"*\"), \n",
    "    lower($\"value\").name(\"lowerCity\"), \n",
    "    (length('value) + 1).as(\"length\"),\n",
    "    lit(\"foo\").alias(\"bar\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости в `select` можно передать список колонок, используя обычные строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withUpper.select(\"value\", \"upperCity\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалить колонку из DF можно с помощью метода `drop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|  Moscow|\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n",
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [value#5]\n",
      "+- Scan ExistingRDD[value#5,upperCity#93]\n"
     ]
    }
   ],
   "source": [
    "// drop не будет выдавать ошибку, если будет указана несуществующая колонка\n",
    "\n",
    "withUpper.drop(\"upperCity\", \"abraKadabra\").show\n",
    "withUpper.show\n",
    "withUpper.localCheckpoint.drop(\"upperCity\", \"abraKadabra\").explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ методы `filter` и `select` принимают в качестве аргументов колонки [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). Это может быть либо ссылка на существующую колонку, либо функцию из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ любые трансформации возвращают новый DF, не меняя существующий\n",
    "+ тип [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) играет важную роль в DF API - на его основе создаются ссылки на существующие колонки, а также функции, принимающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) и возвращающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). По этой причине обычное сравнение `==` не будет работать в DF API, тк `filter` принимает [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column), а не `Boolean`\n",
    "+ Класс DataFrame в последних версиях Spark представляет собой `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`, поэтому его описание следует искать в [org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n",
    "\n",
    "## Очистка данных\n",
    "\n",
    "Одной из задач обработки данных является их очистка. DF API содержит класс функций \"not available\", описанный в пакете [org.apache.spark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions). В данном пакете есть три функции:\n",
    "+ `na.drop`\n",
    "+ `na.fill`\n",
    "+ `na.replace`\n",
    "\n",
    "Для демонстрации работы данных функций создадим новый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "|col                                                                                   |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}|\n",
      "|{ \"name\":\"Madrid\", \"country\":\"Spain\" }                                                |\n",
      "|{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}  |\n",
      "|{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105} |\n",
      "|{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }                      |\n",
      "|{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }  |\n",
      "|{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }  |\n",
      "|{ \"name\":\"New York, \"country\":\"USA\",                                                  |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "|     _corrupt_record|continent|country|     name|population|\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "|                null|   Europe|Rossiya|   Moscow|  12380664|\n",
      "|                null|     null|  Spain|   Madrid|      null|\n",
      "|                null|   Europe| France|    Paris|   2196936|\n",
      "|                null|   Europe|Germany|   Berlin|   3490105|\n",
      "|                null|   Europe|  Spain|Barselona|      null|\n",
      "|                null|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|                null|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|{ \"name\":\"New Yor...|     null|   null|     null|      null|\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testData = \n",
       "raw = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
       "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
       "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
       "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
       "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
       "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
       "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
       "{ \"name\":\"New York, \"country\":\"USA\",\n",
       "jsonStrings:...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val testData =\n",
    "\"\"\"{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"New York, \"country\":\"USA\",\"\"\"\n",
    "\n",
    "// Создаем DF из одной строки и добавляем данные в виде новой колонки\n",
    "val raw = spark.range(0,1).select(lit(testData).alias(\"value\"))\n",
    "\n",
    "// Создаем новую колонку, разибая наши данные по \\n\n",
    "val jsonStrings: Column = split(col(\"value\"), \"\\n\").alias(\"value\")\n",
    "\n",
    "// Используем функцию explode для того, чтобы развернуть массив мехом наружу и используем темную магию \n",
    "// для превращения DataFrame в Dataset[String]\n",
    "val splited: Dataset[String] = raw.select(explode(jsonStrings)).as[String]\n",
    "\n",
    "splited.show(numRows = 10, truncate = false)\n",
    "\n",
    "\n",
    "// Создаем новый датафре... датасет, в котором наши JSON строки будут распарсены\n",
    "val df: Dataset[Row] = spark.read.json(splited)\n",
    "df.printSchema\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для очистки датасета:\n",
    "+ удалим строку с навалидным JSON, сохраним ее в отдельное место\n",
    "+ удалим дубликаты\n",
    "+ заполним `null`ы в колонках\n",
    "+ исправим `Rossiya` на `Russia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corruptData = Array([{ \"name\":\"New York, \"country\":\"USA\",])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array([{ \"name\":\"New York, \"country\":\"USA\",])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corruptData = df.select(col(\"_corrupt_record\")).na.drop(\"all\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#829, country#839, name#206, population#830L], functions=[])\n",
      "+- Exchange hashpartitioning(continent#829, country#839, name#206, population#830L, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#829, country#839, name#206, population#830L], functions=[])\n",
      "      +- *(1) Project [coalesce(continent#204, Undefined) AS continent#829, CASE WHEN (country#205 = Rossiya) THEN Russia ELSE country#205 END AS country#839, name#206, coalesce(population#207L, 0) AS population#830L]\n",
      "         +- *(1) Filter AtLeastNNulls(n, continent#204,country#205,name#206,population#207L)\n",
      "            +- Scan ExistingRDD[continent#204,country#205,name#206,population#207L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fillData = Map(continent -> Undefined, population -> 0)\n",
       "replaceData = Map(Rossiya -> Russia)\n",
       "cleanData = [continent: string, country: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, country: string ... 2 more fields]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fillData: Map[String, Any] = Map(\"continent\" -> \"Undefined\", \"population\" -> 0)\n",
    "val replaceData: Map[Any, Any] = Map(\"Rossiya\" -> \"Russia\")\n",
    "\n",
    "val cleanData = \n",
    "    df\n",
    "    .drop(col(\"_corrupt_record\"))\n",
    "    .localCheckpoint\n",
    "    .na.drop(\"all\")\n",
    "    .na.fill(fillData)\n",
    "    .na.replace(\"country\", replaceData)\n",
    "    .distinct\n",
    "\n",
    "\n",
    "cleanData.explain\n",
    "// .na.drop(\"any\", \"continent\" :: \"country\" :: Nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|Undefined|  Spain|   Madrid|         0|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe|  Spain|Barselona|         0|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanData.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [continent#829, country#839, name#206, population#830L, CASE WHEN ('continent = Europe) THEN 0 WHEN ('continent = Africa) THEN 1 ELSE 2 END AS newCol#938]\n",
      "+- Deduplicate [continent#829, country#839, name#206, population#830L]\n",
      "   +- Project [continent#829, CASE WHEN (country#205 = Rossiya) THEN cast(Russia as string) ELSE country#205 END AS country#839, name#206, population#830L]\n",
      "      +- Project [coalesce(continent#204, cast(Undefined as string)) AS continent#829, country#205, name#206, coalesce(population#207L, cast(0 as bigint)) AS population#830L]\n",
      "         +- Filter AtLeastNNulls(n, continent#204,country#205,name#206,population#207L)\n",
      "            +- LogicalRDD [continent#204, country#205, name#206, population#207L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint, newCol: int\n",
      "Project [continent#829, country#839, name#206, population#830L, CASE WHEN (continent#829 = Europe) THEN 0 WHEN (continent#829 = Africa) THEN 1 ELSE 2 END AS newCol#938]\n",
      "+- Deduplicate [continent#829, country#839, name#206, population#830L]\n",
      "   +- Project [continent#829, CASE WHEN (country#205 = Rossiya) THEN cast(Russia as string) ELSE country#205 END AS country#839, name#206, population#830L]\n",
      "      +- Project [coalesce(continent#204, cast(Undefined as string)) AS continent#829, country#205, name#206, coalesce(population#207L, cast(0 as bigint)) AS population#830L]\n",
      "         +- Filter AtLeastNNulls(n, continent#204,country#205,name#206,population#207L)\n",
      "            +- LogicalRDD [continent#204, country#205, name#206, population#207L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [continent#829, country#839, name#206, population#830L], [continent#829, country#839, name#206, population#830L, CASE WHEN (continent#829 = Europe) THEN 0 WHEN (continent#829 = Africa) THEN 1 ELSE 2 END AS newCol#938]\n",
      "+- Project [coalesce(continent#204, Undefined) AS continent#829, CASE WHEN (country#205 = Rossiya) THEN Russia ELSE country#205 END AS country#839, name#206, coalesce(population#207L, 0) AS population#830L]\n",
      "   +- Filter AtLeastNNulls(n, continent#204,country#205,name#206,population#207L)\n",
      "      +- LogicalRDD [continent#204, country#205, name#206, population#207L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#829, country#839, name#206, population#830L], functions=[], output=[continent#829, country#839, name#206, population#830L, newCol#938])\n",
      "+- Exchange hashpartitioning(continent#829, country#839, name#206, population#830L, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#829, country#839, name#206, population#830L], functions=[], output=[continent#829, country#839, name#206, population#830L])\n",
      "      +- *(1) Project [coalesce(continent#204, Undefined) AS continent#829, CASE WHEN (country#205 = Rossiya) THEN Russia ELSE country#205 END AS country#839, name#206, coalesce(population#207L, 0) AS population#830L]\n",
      "         +- *(1) Filter AtLeastNNulls(n, continent#204,country#205,name#206,population#207L)\n",
      "            +- Scan ExistingRDD[continent#204,country#205,name#206,population#207L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newCol = CASE WHEN (continent = Europe) THEN 0 WHEN (continent = Africa) THEN 1 ELSE 2 END\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CASE WHEN (continent = Europe) THEN 0 WHEN (continent = Africa) THEN 1 ELSE 2 END"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.when\n",
    "\n",
    "val newCol = when(col(\"continent\") === \"Europe\", lit(0))\n",
    "                .when(col(\"continent\") === \"Africa\", lit(1)).otherwise(lit(2))\n",
    "\n",
    "cleanData.withColumn(\"newCol\", newCol).explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myConst = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"3\",\"dataType\":\"integer\"}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myConst = lit(3)\n",
    "myConst.expr.toJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API обладает удобным API для очистки данных, позволяющим разработчику сконцентрироваться разработчику на бизнес логике, а не на написании функций для обработки всех возможных исключительных ситуаций\n",
    "+ метод `spark.read.json` позволяет читать не только файлы, но и `Dataset[String]`, содержащие JSON строки.\n",
    "\n",
    "## Агрегаты\n",
    "Посчитаем суммарное население и количество городов с разбивкой по континентам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#829], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(continent#829, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#829], functions=[partial_count(1)])\n",
      "      +- *(1) Project [continent#829]\n",
      "         +- Scan ExistingRDD[continent#829,country#839,name#206,population#830L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggCount = [continent: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, count: bigint]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggCount = cleanData.localCheckpoint.groupBy('continent).count\n",
    "aggCount.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|continent|sum(population)|\n",
      "+---------+---------------+\n",
      "|   Europe|       18067705|\n",
      "|   Africa|       11922948|\n",
      "|Undefined|              0|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggSum = [continent: string, sum(population): bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, sum(population): bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggSum = cleanData.groupBy('continent).sum(\"population\")\n",
    "aggSum.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы совместить несколько агрегатов в одном DF, мы можем использовать метод `agg()`. Данный метод позволяет использовать любые `Aggregate functions` из пакета [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#829], functions=[count(1), sum(population#830L)])\n",
      "+- Exchange hashpartitioning(continent#829, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#829], functions=[partial_count(1), partial_sum(population#830L)])\n",
      "      +- *(1) Project [continent#829, population#830L]\n",
      "         +- Scan ExistingRDD[continent#829,country#839,name#206,population#830L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "agg = [continent: string, count: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, count: bigint ... 1 more field]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val agg = cleanData.localCheckpoint.groupBy('continent).agg(count(\"*\").alias(\"count\"), sum(\"population\").alias(\"sumPop\"))\n",
    "agg.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью агрегатов мы можем выполнять такие действия, как, например, `collect_list` и `collect_set`. Стоит отметить, что колонки в Spark могут иметь не только скалярные типы, но и структуры, словари и массивы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- countries: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "-RECORD 0-------------------------------------\n",
      " continent | Europe                           \n",
      " countries | [France, Germany, Spain, Russia] \n",
      "-RECORD 1-------------------------------------\n",
      " continent | Africa                           \n",
      " countries | [Egypt]                          \n",
      "-RECORD 2-------------------------------------\n",
      " continent | Undefined                        \n",
      " countries | [Spain]                          \n",
      "\n",
      "== Physical Plan ==\n",
      "ObjectHashAggregate(keys=[continent#829], functions=[collect_list(country#839, 0, 0)])\n",
      "+- Exchange hashpartitioning(continent#829, 200)\n",
      "   +- ObjectHashAggregate(keys=[continent#829], functions=[partial_collect_list(country#839, 0, 0)])\n",
      "      +- *(1) Project [continent#829, country#839]\n",
      "         +- Scan ExistingRDD[continent#829,country#839,name#206,population#830L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggList = [continent: string, countries: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, countries: array<string>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggList = cleanData.localCheckpoint.groupBy('continent).agg(collect_list(\"country\").alias(\"countries\"))\n",
    "aggList.printSchema\n",
    "aggList.show(numRows = 10, truncate = 100, vertical = true)\n",
    "aggList.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы `struct` и `to_json`, мы можем превратить произвольный набор колонок в JSON строку. Этот методы часто используется перед отправкой данных в Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- s: struct (nullable = false)\n",
      " |    |-- continent: string (nullable = false)\n",
      " |    |-- countries: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------------------------------------------+\n",
      "|s                                         |\n",
      "+------------------------------------------+\n",
      "|[Europe, [France, Germany, Spain, Russia]]|\n",
      "|[Africa, [Egypt]]                         |\n",
      "|[Undefined, [Spain]]                      |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withStruct = [s: struct<continent: string, countries: array<string>>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[s: struct<continent: string, countries: array<string>>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withStruct = aggList.select(struct('continent, 'countries).alias(\"s\"))\n",
    "withStruct.printSchema\n",
    "\n",
    "withStruct.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Project [structstojson(named_struct(s, s#1212), Some(Europe/Moscow)) AS s#1419]\n",
      "+- Scan ExistingRDD[s#1212]\n"
     ]
    }
   ],
   "source": [
    "withStruct.localCheckpoint.withColumn(\"s\", to_json(struct(col(\"*\")))).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо превратить все колонки DF в JSON String, можно воспользоваться функций `toJSON`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#1463]\n",
      "+- MapPartitions <function1>, obj#1462: java.lang.String\n",
      "   +- DeserializeToObject createexternalrow(continent#829.toString, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(MapObjects_loopValue5, MapObjects_loopIsNull5, StringType, lambdavariable(MapObjects_loopValue5, MapObjects_loopIsNull5, StringType, true).toString, countries#1161, None).array, true, false), StructField(continent,StringType,false), StructField(countries,ArrayType(StringType,true),true)), obj#1461: org.apache.spark.sql.Row\n",
      "      +- Scan ExistingRDD[continent#829,countries#1161]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jString = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jString: Dataset[String] = aggList.localCheckpoint.toJSON\n",
    "jString.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нам необходимо создать колонки из значений текущих колонок, мы можем воспользоваться функцией `pivot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+---------+\n",
      "|country|  Africa|  Europe|Undefined|\n",
      "+-------+--------+--------+---------+\n",
      "| Russia|    null|12380664|     null|\n",
      "|Germany|    null| 3490105|     null|\n",
      "| France|    null| 2196936|     null|\n",
      "|  Spain|    null|       0|        0|\n",
      "|  Egypt|11922948|    null|     null|\n",
      "+-------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanData.groupBy(col(\"country\")).pivot(\"continent\").agg(sum(\"population\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API позволяет строить большое количество агрегатов. При этом необходимо помнить, что операции `groupBy`, `cube`, `rollup` возвращают [org.apache.spark.sql.RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset), к которому затем необходимо применить одну из функций агрегации - `count`, `sum`, `agg` и т. п.\n",
    "+ При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере.\n",
    "\n",
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несколько агрегатов. Несмотря на то, что `onlyRu` является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " ident        | RU-0006               \n",
      " type         | closed                \n",
      " name         | Arabatuk Air Base     \n",
      " elevation_ft | 2280                  \n",
      " continent    | EU                    \n",
      " iso_country  | RU                    \n",
      " iso_region   | RU-CHI                \n",
      " municipality | Daurija               \n",
      " gps_code     | null                  \n",
      " iata_code    | null                  \n",
      " local_code   | ZA2N                  \n",
      " coordinates  | 117.098999, 50.223801 \n",
      "only showing top 1 row\n",
      "\n",
      "+---------------+-----+\n",
      "|   municipality|count|\n",
      "+---------------+-----+\n",
      "|          Chita|    3|\n",
      "|   Nizhneudinsk|    2|\n",
      "|       Ulan Ude|    2|\n",
      "|  Nizhneangarsk|    2|\n",
      "|        Irkutsk|    2|\n",
      "|         Borzya|    2|\n",
      "|  Kazachinskaya|    1|\n",
      "|   Snezhnogorsk|    1|\n",
      "|   Karachayevsk|    1|\n",
      "|          Kyren|    1|\n",
      "|           Tura|    1|\n",
      "|         Amazar|    1|\n",
      "|          Baley|    1|\n",
      "|   Novokuznetsk|    1|\n",
      "|       Barguzin|    1|\n",
      "|     Akkem Lake|    1|\n",
      "|     Ust-Ilimsk|    1|\n",
      "|Severo-Eniseysk|    1|\n",
      "|    Olovyannaya|    1|\n",
      "|        Chistyy|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "onlyRuAndHigh = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val onlyRuAndHigh = airports.filter('iso_country === \"RU\" and 'elevation_ft > 1000)\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.count\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " ident        | RU-0006               \n",
      " type         | closed                \n",
      " name         | Arabatuk Air Base     \n",
      " elevation_ft | 2280                  \n",
      " continent    | EU                    \n",
      " iso_country  | RU                    \n",
      " iso_region   | RU-CHI                \n",
      " municipality | Daurija               \n",
      " gps_code     | null                  \n",
      " iata_code    | null                  \n",
      " local_code   | ZA2N                  \n",
      " coordinates  | 117.098999, 50.223801 \n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+-----+\n",
      "|        municipality|count|\n",
      "+--------------------+-----+\n",
      "|               Chita|    3|\n",
      "|        Nizhneudinsk|    2|\n",
      "|            Ulan Ude|    2|\n",
      "|       Nizhneangarsk|    2|\n",
      "|             Irkutsk|    2|\n",
      "|              Borzya|    2|\n",
      "|     Severo-Eniseysk|    1|\n",
      "|             Chistyy|    1|\n",
      "|        Karachayevsk|    1|\n",
      "|            Barguzin|    1|\n",
      "|              Amazar|    1|\n",
      "|   Usolye-Sibirskoye|    1|\n",
      "|               Baley|    1|\n",
      "|            Ekimchan|    1|\n",
      "|          Akkem Lake|    1|\n",
      "|               Kyren|    1|\n",
      "|          Ust-Ilimsk|    1|\n",
      "|         Olovyannaya|    1|\n",
      "|       Kazachinskaya|    1|\n",
      "|Zheleznogorsk-Ili...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyRuAndHigh.cache\n",
    "onlyRuAndHigh.count\n",
    "// при вычислении count данные будут помещены в cache\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").show\n",
    "\n",
    "onlyRuAndHigh.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyRuAndHigh.localCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Репартиционирование\n",
    "RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями. В графе вычисления, который называется в Spark DAG (Direct Acyclic Graph), есть три основных компонента - `job`, `stage`, `task`.\n",
    "\n",
    "`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.\n",
    "\n",
    "Исходя из этого, важно обеспечивать:\n",
    "+ достаточное количество партиций для распределения нагрузки по всем воркерам\n",
    "+ равномерное распределение данных между партициями\n",
    "\n",
    "Создадим датасет с перекосом данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|0               |\n",
      "|900             |\n",
      "|0               |\n",
      "|100             |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "skewColumn = CASE WHEN (id < 900) THEN 0 ELSE 1 END\n",
       "skewDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "printItemPerPartition: [T](ds: org.apache.spark.sql.Dataset[T])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val skewColumn = when(col(\"id\") < 900, lit(0)).otherwise(lit(1))\n",
    "\n",
    "val skewDf = spark.range(0,1000).repartition(10, skewColumn)\n",
    "\n",
    "def printItemPerPartition[T](ds: Dataset[T]): Unit = {\n",
    "    ds.mapPartitions { x => Iterator(x.length) }\n",
    "    .withColumnRenamed(\"value\", \"itemPerPartition\")\n",
    "    .show(50, false)\n",
    "}\n",
    "\n",
    "printItemPerPartition[java.lang.Long](skewDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любые операции с таким датасетом будут работать медленно, т.к.\n",
    "+ если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать\n",
    "+ из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом\n",
    "\n",
    "Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.\n",
    "\n",
    "Для устранения проблемы перекоса данных, следует использовать метод `repartition`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|49              |\n",
      "|49              |\n",
      "|49              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|51              |\n",
      "|50              |\n",
      "|51              |\n",
      "|52              |\n",
      "|51              |\n",
      "|51              |\n",
      "|51              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|49              |\n",
      "|49              |\n",
      "|49              |\n",
      "|49              |\n",
      "+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Exchange RoundRobinPartitioning(20)\n",
      "+- *(1) Range (0, 1000, step=1, splits=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "repartitionedDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20)\n",
    "\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)\n",
    "skewDf.repartition(20).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|37              |\n",
      "|61              |\n",
      "|48              |\n",
      "|59              |\n",
      "|47              |\n",
      "|54              |\n",
      "|45              |\n",
      "|58              |\n",
      "|55              |\n",
      "|55              |\n",
      "|56              |\n",
      "|46              |\n",
      "|45              |\n",
      "|46              |\n",
      "|49              |\n",
      "|64              |\n",
      "|44              |\n",
      "|39              |\n",
      "|40              |\n",
      "|52              |\n",
      "+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(id#2181L, 20)\n",
      "+- *(1) Range (0, 1000, step=1, splits=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "repartitionedDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "// поэтому Spark выполнит HashPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20, col(\"id\"))\n",
    "\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)\n",
    "repartitionedDf.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|900             |\n",
      "|100             |\n",
      "|0               |\n",
      "+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Coalesce 3\n",
      "+- Exchange hashpartitioning(CASE WHEN (id#2181L < 900) THEN 0 ELSE 1 END, 10)\n",
      "   +- *(1) Range (0, 1000, step=1, splits=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "coalescedDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val coalescedDf = skewDf.coalesce(3)\n",
    "printItemPerPartition[java.lang.Long](coalescedDf)\n",
    "coalescedDf.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://pngimage.net/wp-content/uploads/2018/06/соленья-png-4.png\">\n",
    "\n",
    "### Соленья\n",
    "Часто при вычислении агрегатов приходится работать с перекошенными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "+--------------+-----+\n",
      "|          type|count|\n",
      "+--------------+-----+\n",
      "| small_airport|34369|\n",
      "|      heliport|11507|\n",
      "|medium_airport| 4537|\n",
      "|        closed| 4154|\n",
      "| seaplane_base| 1020|\n",
      "| large_airport|  616|\n",
      "|   balloonport|   23|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.printSchema\n",
    "\n",
    "airports.groupBy('type).count.orderBy('count.desc).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный `HashPartitioning` по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      " salt         | 8                                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "saltModTen = CAST(pmod(round((rand(3711221825669158006) * 100), 0), 10) AS INT)\n",
       "salted = [ident: string, type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 11 more fields]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val saltModTen = pmod(round((rand() * 100), 0), lit(10)).cast(\"int\")\n",
    "\n",
    "val salted = airports.withColumn(\"salt\", saltModTen)\n",
    "salted.show(numRows = 1, truncate = 200, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-----+\n",
      "|type          |salt|count|\n",
      "+--------------+----+-----+\n",
      "|small_airport |6   |3527 |\n",
      "|small_airport |9   |3494 |\n",
      "|small_airport |3   |3492 |\n",
      "|small_airport |1   |3479 |\n",
      "|small_airport |4   |3460 |\n",
      "|small_airport |7   |3448 |\n",
      "|small_airport |5   |3436 |\n",
      "|small_airport |8   |3375 |\n",
      "|small_airport |2   |3329 |\n",
      "|small_airport |0   |3329 |\n",
      "|heliport      |7   |1179 |\n",
      "|heliport      |5   |1177 |\n",
      "|heliport      |1   |1165 |\n",
      "|heliport      |3   |1162 |\n",
      "|heliport      |8   |1152 |\n",
      "|heliport      |2   |1148 |\n",
      "|heliport      |4   |1148 |\n",
      "|heliport      |9   |1139 |\n",
      "|heliport      |6   |1129 |\n",
      "|heliport      |0   |1108 |\n",
      "|medium_airport|7   |490  |\n",
      "|medium_airport|1   |476  |\n",
      "|medium_airport|5   |461  |\n",
      "|medium_airport|6   |456  |\n",
      "|medium_airport|9   |453  |\n",
      "|medium_airport|0   |453  |\n",
      "|medium_airport|2   |446  |\n",
      "|closed        |2   |438  |\n",
      "|medium_airport|3   |437  |\n",
      "|medium_airport|4   |435  |\n",
      "|closed        |0   |434  |\n",
      "|medium_airport|8   |430  |\n",
      "|closed        |5   |427  |\n",
      "|closed        |9   |420  |\n",
      "|closed        |3   |419  |\n",
      "|closed        |7   |414  |\n",
      "|closed        |1   |412  |\n",
      "|closed        |8   |402  |\n",
      "|closed        |4   |398  |\n",
      "|closed        |6   |390  |\n",
      "|seaplane_base |7   |114  |\n",
      "|seaplane_base |5   |112  |\n",
      "|seaplane_base |2   |108  |\n",
      "|seaplane_base |6   |107  |\n",
      "|seaplane_base |9   |103  |\n",
      "|seaplane_base |0   |101  |\n",
      "|seaplane_base |8   |100  |\n",
      "|seaplane_base |3   |99   |\n",
      "|seaplane_base |1   |89   |\n",
      "|seaplane_base |4   |87   |\n",
      "|large_airport |0   |79   |\n",
      "|large_airport |5   |72   |\n",
      "|large_airport |7   |67   |\n",
      "|large_airport |9   |64   |\n",
      "|large_airport |8   |63   |\n",
      "|large_airport |1   |62   |\n",
      "|large_airport |3   |58   |\n",
      "|large_airport |6   |55   |\n",
      "|large_airport |2   |49   |\n",
      "|large_airport |4   |47   |\n",
      "|balloonport   |1   |7    |\n",
      "|balloonport   |4   |3    |\n",
      "|balloonport   |3   |3    |\n",
      "|balloonport   |0   |2    |\n",
      "|balloonport   |8   |2    |\n",
      "|balloonport   |5   |2    |\n",
      "|balloonport   |2   |1    |\n",
      "|balloonport   |6   |1    |\n",
      "|balloonport   |7   |1    |\n",
      "|balloonport   |9   |1    |\n",
      "+--------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "firstStep = [type: string, salt: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, salt: int ... 1 more field]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstStep = salted.groupBy('type, 'salt).count()\n",
    "\n",
    "firstStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторым шагом мы делаем еще один агрегат, суммируя предыдущие значения `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|type          |count|\n",
      "+--------------+-----+\n",
      "|small_airport |34369|\n",
      "|heliport      |11507|\n",
      "|medium_airport|4537 |\n",
      "|closed        |4154 |\n",
      "|seaplane_base |1020 |\n",
      "|large_airport |616  |\n",
      "|balloonport   |23   |\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "secondStep = [type: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, count: bigint]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val secondStep = firstStep.groupBy('type).agg(sum(\"count\").alias(\"count\"))\n",
    "\n",
    "secondStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что мы сделали две группировки вместо одной, распределение данных по воркерам было более равномерным, что позволило избежать OOM на воркерах.\n",
    "\n",
    "### Выводы:\n",
    "+ Партиционирование - важный аспект распределенных вычислений, от которого напрямую зависит стабильность и скорость вычислений\n",
    "+ В Spark всегда работает правило 1 TASK = 1 THREAD = 1 PARTITION\n",
    "+ Репартиционирование и соление данных позволяет решить проблему перекоса данных и вычислений\n",
    "+ Важно помнить, что репартиционирование использует дисковую и сетевую подсистемы - обмен данными происходит **по сети**, а результат записывается **на диск**, что может стать узким местом при выполнении репартиционирования\n",
    "\n",
    "## Встроенные функции\n",
    "Помимо базовых SQL операторов, в Spark существует большой набор встроенных функций:\n",
    "+ API методы из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ [SQL built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|pmod|\n",
      "+---+----+\n",
      "|  0|   0|\n",
      "|  1|   1|\n",
      "|  2|   0|\n",
      "|  3|   1|\n",
      "|  4|   0|\n",
      "|  5|   1|\n",
      "|  6|   0|\n",
      "|  7|   1|\n",
      "|  8|   0|\n",
      "|  9|   1|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "newCol = pmod(id, 2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pmod(id, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(0,10)\n",
    "\n",
    "// используем org.apache.spark.sql.functions\n",
    "val newCol: Column = pmod(col(\"id\"), lit(2))\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|pmod|\n",
      "+---+----+\n",
      "|  0|   0|\n",
      "|  1|   1|\n",
      "|  2|   0|\n",
      "|  3|   1|\n",
      "|  4|   0|\n",
      "|  5|   1|\n",
      "|  6|   0|\n",
      "|  7|   1|\n",
      "|  8|   0|\n",
      "|  9|   1|\n",
      "+---+----+\n",
      "\n",
      "root\n",
      " |-- pmod: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newCol = pmod(id, 2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pmod(id, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "// используем SQL built-in functions\n",
    "val newCol: Column = expr(\"\"\"pmod(id, 2)\"\"\")\n",
    "df.withColumn(\"pmod\", newCol).show\n",
    "df.withColumn(\"pmod\", newCol).select('pmod.cast(\"string\")).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: id, id, id.;\n"
     ]
    }
   ],
   "source": [
    "import scala.util.{Try, Failure, Success}\n",
    "\n",
    "Try { spark.range(10).select(lit(0).alias(\"id\"), lit(1).alias(\"id\"), lit(\"a\").alias(\"id\")).select('id) } match { \n",
    "    case Success(a) => println(a.show)\n",
    "    case Failure(e) => println(e)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Spark обладает широким набором функций для работы с колонками разных типов, включая простые типы - строки, числа, и т. д., а также словари, массивы и структуры\n",
    "+ Встроенные функции принимают колонки `org.apache.spark.sql.Column` и возвращают `org.apache.spark.sql.Column` в большинстве случаев\n",
    "+ Встроенные функции доступны в двух местах - org.apache.spark.sql.functions и SQL built-in functions\n",
    "+ Встроенные функции можно (и нужно) использовать вместе - на вход во встроенные функции могут подаваться результаты встроенной функции, тк все они возвращают `sql.Column` \n",
    "\n",
    "### Пользовательские функции\n",
    "\n",
    "В том случае, если функционала встроенных функций не хватает, можно написать пользовательскую функцию - UDF. Пользовательская функция может принимать до 16 аргументов. Соответствие Spark и Scala типов описано [здесь](https://spark.apache.org/docs/latest/sql-reference.html#data-types)\n",
    "\n",
    "Необходимо помнить, что `null` в Spark превращается в `null` внутри UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|id |idPlusOne|\n",
      "+---+---------+\n",
      "|0  |1        |\n",
      "|1  |2        |\n",
      "|2  |3        |\n",
      "|3  |4        |\n",
      "|4  |5        |\n",
      "|5  |6        |\n",
      "|6  |7        |\n",
      "|7  |8        |\n",
      "|8  |9        |\n",
      "|9  |10       |\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "plusOne = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,LongType,Some(List(LongType)))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val plusOne = udf { (value: Long) => value + 1 }\n",
    "\n",
    "df.withColumn(\"idPlusOne\", plusOne(col(\"id\"))).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользовательская функция может возвращать:\n",
    "+ простой тип - `String`, `Long`, `Float`, `Boolean` и т.д.\n",
    "+ массив - любые коллекции, наследующие `Seq[T]` - `List[T]`, `Vector[T]` и т. д.\n",
    "+ словарь - `Map[A,B]`\n",
    "+ инстанс `case class`'а\n",
    "+ Option[T]\n",
    "\n",
    "Реализуем функцию, которая возвращает имя хоста, на котором работает воркер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|id |hostname      |\n",
      "+---+--------------+\n",
      "|0  |t3nq-wks.local|\n",
      "|1  |t3nq-wks.local|\n",
      "|2  |t3nq-wks.local|\n",
      "|3  |t3nq-wks.local|\n",
      "|4  |t3nq-wks.local|\n",
      "|5  |t3nq-wks.local|\n",
      "|6  |t3nq-wks.local|\n",
      "|7  |t3nq-wks.local|\n",
      "|8  |t3nq-wks.local|\n",
      "|9  |t3nq-wks.local|\n",
      "+---+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hostname = UserDefinedFunction(<function0>,StringType,Some(List()))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function0>,StringType,Some(List()))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.InetAddress\n",
    "\n",
    "val hostname = udf { () => InetAddress.getLocalHost().getHostName() }\n",
    "\n",
    "df.withColumn(\"hostname\", hostname()).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем использовать монады `Try[T]` и `Option[T]` и для написания пользовательской функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- divideTwoBy: long (nullable = true)\n",
      "\n",
      "+---+-----------+\n",
      "|id |divideTwoBy|\n",
      "+---+-----------+\n",
      "|0  |null       |\n",
      "|1  |2          |\n",
      "|2  |1          |\n",
      "|3  |0          |\n",
      "|4  |0          |\n",
      "|5  |0          |\n",
      "|6  |0          |\n",
      "|7  |0          |\n",
      "|8  |0          |\n",
      "|9  |0          |\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "divideTwoBy = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n",
       "result = [id: bigint, divideTwoBy: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, divideTwoBy: bigint]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Try\n",
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val divideTwoBy = udf { (inputValue: Long) => Try(2L / inputValue).toOption }\n",
    "\n",
    "val result = df.withColumn(\"divideTwoBy\", divideTwoBy(col(\"id\")))\n",
    "result.printSchema\n",
    "result.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).select('id, lit(\"a\").alias(\"foo\")).drop(\"foo\")\n",
    "spark.range(10).withColumn(\"id\", 'id + 1)\n",
    "spark.range(10).select(('id + 1).alias(\"id\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Пользовательские функции позволяют реализовать произвольный алгоритм и использовать его в DF API\n",
    "+ Пользовательские функции работают медленнее встроенных, поскольку при использовании встроенных функций Spark использует ряд оптимизаций, например векторизацию вычислений на уровне CPU\n",
    "\n",
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "По методу соединения join'ы бывают:\n",
    "![Joins](http://kirillpavlov.com/images/join-types.png)\n",
    "[Источник](http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/)\n",
    "\n",
    "Добавим новую колонку к датасету `airports`, в которой будет процент заданного типа аэропорта ко всем типам аэропорта по каждой стране. Первым шагом посчитаем число аэропортов каждого типа по стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+\n",
      "|type          |iso_country|cnt_country_type|\n",
      "+--------------+-----------+----------------+\n",
      "|large_airport |GB         |27              |\n",
      "|small_airport |MP         |1               |\n",
      "|heliport      |CH         |19              |\n",
      "|closed        |LT         |4               |\n",
      "|medium_airport|SS         |3               |\n",
      "+--------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggTypeCountry = [type: string, iso_country: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, iso_country: string ... 1 more field]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{count, round, lit}\n",
    "\n",
    "val aggTypeCountry = airports.groupBy('type, 'iso_country).agg(count(\"*\").alias(\"cnt_country_type\"))\n",
    "\n",
    "aggTypeCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем количество аэропортов по каждой стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|iso_country|cnt_country|\n",
      "+-----------+-----------+\n",
      "|DZ         |61         |\n",
      "|LT         |59         |\n",
      "|MM         |75         |\n",
      "|CI         |26         |\n",
      "|TC         |8          |\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggCountry = [iso_country: string, cnt_country: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, cnt_country: bigint]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggCountry = airports.groupBy('iso_country).agg(count(\"*\").alias(\"cnt_country\"))\n",
    "aggCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим получившиеся датасеты и получим процентное распределение типов аэропорта по стране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------+\n",
      "|iso_country|type          |percent|\n",
      "+-----------+--------------+-------+\n",
      "|GB         |large_airport |2.33   |\n",
      "|MP         |small_airport |9.09   |\n",
      "|CH         |heliport      |21.84  |\n",
      "|LT         |closed        |6.78   |\n",
      "|SS         |medium_airport|6.52   |\n",
      "+-----------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "percent = [iso_country: string, type: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, type: string ... 1 more field]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val percent = \n",
    "    aggTypeCountry\n",
    "        .join(aggCountry, \"iso_country\" :: Nil, \"inner\")\n",
    "        .select('iso_country, 'type, (round(lit(100) * 'cnt_country_type / 'cnt_country, 2).alias(\"percent\")))\n",
    "percent.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим полученный датасет с изначальным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+-------+\n",
      "|ident  |iso_country|type          |percent|\n",
      "+-------+-----------+--------------+-------+\n",
      "|EGAC   |GB         |large_airport |2.33   |\n",
      "|EGCN   |GB         |large_airport |2.33   |\n",
      "|EGFF   |GB         |large_airport |2.33   |\n",
      "|EGGP   |GB         |large_airport |2.33   |\n",
      "|EGGW   |GB         |large_airport |2.33   |\n",
      "|EGLL   |GB         |large_airport |2.33   |\n",
      "|EGPH   |GB         |large_airport |2.33   |\n",
      "|LCRA   |GB         |large_airport |2.33   |\n",
      "|EYKR   |LT         |closed        |6.78   |\n",
      "|LT-0001|LT         |closed        |6.78   |\n",
      "|LT-0002|LT         |closed        |6.78   |\n",
      "|LSHI   |CH         |heliport      |21.84  |\n",
      "|LSXL   |CH         |heliport      |21.84  |\n",
      "|LSXM   |CH         |heliport      |21.84  |\n",
      "|LSXP   |CH         |heliport      |21.84  |\n",
      "|LSXU   |CH         |heliport      |21.84  |\n",
      "|ZLG    |EH         |closed        |14.29  |\n",
      "|NZCH   |NZ         |large_airport |1.42   |\n",
      "|SPIM   |PE         |large_airport |1.14   |\n",
      "|EBAW   |BE         |medium_airport|5.48   |\n",
      "+-------+-----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [iso_country: string, type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, type: string ... 11 more fields]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = airports.join(percent, \"iso_country\" :: \"type\" :: Nil, \"left\")\n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех наших джойнах присутствует массив `Seq[String]`. Это синтаксических сахар, позволяющий не переименовывать колонки датасетов, а просто указать, что соединение будет делаться по колонкам с именами, входящим в массив.\n",
    "\n",
    "В общем случае условие джойна должно быть выражено в виде колонки `sql.Column`, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "// val joinCondition: Column = col(\"left_a\") === col(\"right_a\") and col(\"left_b\") === col(\"right_b\")\n",
    "\n",
    "val joinCondition = expr(\"\"\" left.a = right.a and left.b = right.b \"\"\")\n",
    "left.alias(\"left\").join(right.alias(\"right\"), joinCondition, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left = [id: bigint, foo: string]\n",
       "right = [id: bigint, foo: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[left_foo: string, right_foo: string]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val left = spark.range(10).withColumn(\"foo\", lit(\"foo\"))\n",
    "val right = spark.range(10).withColumn(\"foo\", lit(\"foo\"))\n",
    "\n",
    "left.join(right, \"id\" :: Nil).select(left(\"foo\").alias(\"left_foo\"), right(\"foo\").alias(\"right_foo\"))\n",
    "\n",
    "// python - df[\"foo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом в данном выражении допускается использование встроенных функций, пользовательских функций и операторов сравнения. Однако следует помнить, что мы выполняем джойн двух распределенных датасетов и если условие соединения будет плохо составлено, то Spark выполнит `cross join`, производительность которого будет \"крайне мала\" &copy;\n",
    "\n",
    "### Выводы:\n",
    "+ Spark поддерживает большое число типов соединений\n",
    "+ Условием соединения может быть `Seq[String]`, либо `sql.Column`\n",
    "+ При использовании сложных условий соединения следует избегать тех, которые приведут к `cross join`\n",
    "\n",
    "## Оконные функции\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [org.apache.spark.sql.expressions.Window](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```val window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series данными.\n",
    "\n",
    "Выполним задачу с вычисление процента отношения типов аэропортов, используя оконные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+-------+\n",
      "|ident  |iso_country|type          |percent|\n",
      "+-------+-----------+--------------+-------+\n",
      "|DZ-0003|DZ         |closed        |3.28   |\n",
      "|LT-0001|LT         |closed        |6.78   |\n",
      "|LT-0013|LT         |closed        |6.78   |\n",
      "|VYSO   |MM         |closed        |1.33   |\n",
      "|EYPH   |LT         |heliport      |3.39   |\n",
      "|DAAG   |DZ         |large_airport |1.64   |\n",
      "|EYVI   |LT         |large_airport |1.69   |\n",
      "|DAAB   |DZ         |medium_airport|59.02  |\n",
      "|DAAD   |DZ         |medium_airport|59.02  |\n",
      "|DAAE   |DZ         |medium_airport|59.02  |\n",
      "|DAUB   |DZ         |medium_airport|59.02  |\n",
      "|DAUG   |DZ         |medium_airport|59.02  |\n",
      "|DAUK   |DZ         |medium_airport|59.02  |\n",
      "|EYKD   |LT         |medium_airport|10.17  |\n",
      "|EYPP   |LT         |medium_airport|10.17  |\n",
      "|VYEL   |MM         |medium_airport|26.67  |\n",
      "|VYKG   |MM         |medium_airport|26.67  |\n",
      "|VYMO   |MM         |medium_airport|26.67  |\n",
      "|VYSW   |MM         |medium_airport|26.67  |\n",
      "|VYTD   |MM         |medium_airport|26.67  |\n",
      "+-------+-----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "windowCountry = org.apache.spark.sql.expressions.WindowSpec@72ff8eaa\n",
       "windowTypeCountry = org.apache.spark.sql.expressions.WindowSpec@19b51dbd\n",
       "result = [ident: string, type: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 13 more fields]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val windowCountry = Window.partitionBy(\"iso_country\")\n",
    "val windowTypeCountry = Window.partitionBy(\"type\", \"iso_country\")\n",
    "\n",
    "val result = airports.localCheckpoint\n",
    "                .withColumn(\"cnt_country\", count(\"*\").over(windowCountry))\n",
    "                .withColumn(\"cnt_country_type\", count(\"*\").over(windowTypeCountry))\n",
    "                .withColumn(\"percent\", round(lit(100) * 'cnt_country_type / 'cnt_country, 2))\n",
    "                            \n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [rn#3127, ident#1652]\n",
      "+- Window [row_number() windowspecdefinition(ident#1652 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#3127], [ident#1652 ASC NULLS FIRST]\n",
      "   +- *(2) Sort [ident#1652 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange SinglePartition\n",
      "         +- *(1) FileScan csv [ident#1652] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string>\n"
     ]
    }
   ],
   "source": [
    "airports.withColumn(\"rn\", row_number().over(Window.partitionBy().orderBy(\"ident\"))).select('rn, 'ident).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [ident#1652, type#1653, name#1654, elevation_ft#1655, continent#1656, iso_country#1657, iso_region#1658, municipality#1659, gps_code#1660, iata_code#1661, local_code#1662, coordinates#1663, cnt_country#2950L, cnt_country_type#2966L, round((cast((100 * cnt_country_type#2966L) as double) / cast(cnt_country#2950L as double)), 2) AS percent#2981]\n",
      "+- Window [count(1) windowspecdefinition(type#1653, iso_country#1657, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS cnt_country_type#2966L], [type#1653, iso_country#1657]\n",
      "   +- *(2) Sort [type#1653 ASC NULLS FIRST, iso_country#1657 ASC NULLS FIRST], false, 0\n",
      "      +- Window [count(1) windowspecdefinition(iso_country#1657, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS cnt_country#2950L], [iso_country#1657]\n",
      "         +- *(1) Sort [iso_country#1657 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(iso_country#1657, 200)\n",
      "               +- Scan ExistingRDD[ident#1652,type#1653,name#1654,elevation_ft#1655,continent#1656,iso_country#1657,iso_region#1658,municipality#1659,gps_code#1660,iata_code#1661,local_code#1662,coordinates#1663]\n"
     ]
    }
   ],
   "source": [
    "result.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Оконные функции позволяют применять функции, применительно к окнам данных\n",
    "+ Окно определяется списком колонок и сортировкой\n",
    "+ Применение оконных функций приводит к `shuffle`\n",
    "\n",
    "После завершения работы не забывайте останавливать `SparkSession`, чтобы освободить ресурсы кластера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
