{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Structured Streaming III\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Foreach Batch Sink\n",
    "+ FAIR Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreach Batch Sink\n",
    "\n",
    "### В чем проблема обычных \"синков\" ?\n",
    "\n",
    "Подготовим вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSink(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"chk/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def airportsDf() = {\n",
    "    val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "    spark.read.options(csvOptions).csv(\"datasets/airport-codes.csv\")\n",
    "}\n",
    "\n",
    "def randomIdent() = {\n",
    "    \n",
    "    val idents = airportsDf().select('ident).limit(20).distinct.as[String].collect\n",
    "\n",
    "    val columnArray = idents.map( x => lit(x) )\n",
    "    val sparkArray = array(columnArray:_*)\n",
    "    val shuffledArray = shuffle(sparkArray)\n",
    "\n",
    "    shuffledArray(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим старые чекпоинты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"rm -rf chk\".!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и запустим два стрима на основе одного датафрейма `myStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val myStream = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state1\", myStream).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state1\", myStream).start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При запуске второго стрима мы получаем ошибку, связанную с попыткой переиспользования активного чекпоинта. Запустим второй стрим с новым чекпоинтом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConsoleSink(\"state2\", myStream).start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный эксперимент показывает, что два стрима, даже если созданы на основе одного датафрейма, не могут использовать общий чекпоинт, а значит:\n",
    "- они работают независимо\n",
    "- чтение данных из источника происходит у каждого стрима отдельно\n",
    "- один стрим может начать отставать (лагать) от другого стрима\n",
    "\n",
    "Остановим запущенные стримы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**foreachBatch** синк - это синк, позволяющий применять **произвольную** функцию к каждому батчу в стриме, работая с ним, как со **статическим** датафреймом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "def createSink(chkName: String, df: DataFrame)(batchFunc: (DataFrame, Long) => Unit) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"chk/$chkName\")\n",
    "    .foreachBatch(batchFunc)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "createSink(\"state4\", myStream) { (df, id) => \n",
    "    df.show(1, false)\n",
    "    println(s\"This is batch $id\")\n",
    "}.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Обычные синки позволяют создавать простые пайплайны; в реальной жизни чаще используется `foreachBatch`\n",
    "- `foreachBatch` позволяет:\n",
    "  + Можно использовать любое API внутри функции\n",
    "  + Можно использовать `cache()` и `persist()`\n",
    "  + Можно выполнять запись в несколько разных мест\n",
    "  + Поддерживает режимы `append`, `update`, `complete`\n",
    "  + Использовать `batch id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAIR Scheduler\n",
    "До этого момента мы всегда action'ы над датафреймами один из другим, то есть последовательно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(0,100)\n",
    "df.count\n",
    "df.show\n",
    "df.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный подход имеет существенный недостаток - низкая утилизация ресурсов, предоставленных Spark приложению, поскольку каждое действие блокирует основной поток на драйвере и не позволяет выполняться следующим, даже если у приложения еще есть свободные ресурсы.\n",
    "\n",
    "Вышеописанная проблема не является критичной для обычных ETL приложений, однако в стримах, где задержка обработки данных является одним из ключевых параметров, простой выделенных ресурсов недопустим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val delay = udf { () => Thread.sleep(1000); true}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(0,30, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"foo\", delay()).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если запустить вычисление данного датафрейма несколько раз, то он будет выполняться последовательно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    1 to 5 foreach { _ => \n",
    "        val df = spark.range(0, 10, 1, 1)\n",
    "        df.withColumn(\"foo\", delay()).collect\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя метод `par`, мы можем запустить вычисление на драйвере одновременно для всех датафреймов, однако планировщик Spark приложения работает в режиме FIFO, поэтому на воркерах партиции разных датафреймов будут все еще обрабатываться последовательно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    (1 to 5).par.foreach { _ => \n",
    "        val df = spark.range(0, 10, 1, 1)\n",
    "        df.withColumn(\"foo\", delay()).collect\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, если переключить режим планировщика в `FAIR`, то все 5 действий будут выполняться параллельно. Для этого необходимо установить две опции:\n",
    "```\n",
    "spark.scheduler.mode FAIR\n",
    "spark.scheduler.allocation.file /path/to/fairscheduler.xml\n",
    "```\n",
    "Файл `fairscheduler.xml` должен содержать:\n",
    "```xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<allocations>\n",
    "  <pool name=\"default\">\n",
    "    <schedulingMode>FAIR</schedulingMode>\n",
    "    <weight>1</weight>\n",
    "  </pool>\n",
    "</allocations>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнив перезапуск Spark приложения с новыми параметрами (в этом месте необходимо перезапустить kernel или весь jupyter), можно убедиться, что код ниже отработает гораздо быстрее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "    (1 to 5).par.foreach { _ => \n",
    "        val df = spark.range(0, 10, 1, 1)\n",
    "        df.withColumn(\"foo\", delay()).collect\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет запускать действия параллельно\n",
    "- Для параллельного запуска действий должны быть выполнены следующие условия:\n",
    "  + Планировщик Spark должен быть переведен в FAIR режим\n",
    "  + Пул `default` (должен быть переведен в FAIR режим)\n",
    "  + Действия должны запускаться на драйвере паралелльно (можно использовать любое API, поддерживающее multithreading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
