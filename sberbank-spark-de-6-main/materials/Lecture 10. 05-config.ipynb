{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Настройка, мониторинг и оптимизация Spark\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Настройка spark-submit окружения\n",
    "+ Работа со Spark UI\n",
    "+ \"Вредные\" советы\n",
    "\n",
    "## Настройка spark-submit окружения\n",
    "Дистрибутив Spark содержит в себе различные библиотеки, примеры конфигурационных файлов и набор утилит. Любое Spark приложение запускается одной из утилит:\n",
    "- spark-shell - запуска интерактивного `Scala REPL` с поддержкой Spark\n",
    "- pyspark - запуск интеракивного `python` шела с поддержкой Spark\n",
    "- spark-submit - запуск Spark приложений, собранных в виде `jar` или `py` файла с зависимостями\n",
    "\n",
    "### Структура дистрибутива Spark\n",
    "Скачаем последнюю версию с официального сайта:\n",
    "https://spark.apache.org/downloads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "println(\"wget https://apache-mirror.rbc.ru/pub/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz -P lib/\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распакуем `tar.gz` архив:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"tar xvf lib/spark-2.4.5-bin-hadoop2.7.tgz -C lib/\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архив теперь можно удалить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"rm -f lib/spark-2.4.5-bin-hadoop2.7.tgz\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим содержимое распакованного дистрибутива:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark-submit`, `pyspark`, `spark-shell` находятся в каталоге `bin`. Когда вы запускаете свои Spark задачи, под капотом используются именно эти утилиты. Достаточно, чтобы они были установлены только на те хосты, с которых происходит запуск приложений. На обычных узлах кластера они не нужны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/bin\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каталоге `python/lib` находятся зависимости для `python` - когда вы натраиваете у себя среду разработки, не нужно ставить pyspark в свой venv. Правильнее добавить эти библиотеки из дистрибутива. Так вы избежите возможных конфликтов версий в будущем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/python/lib\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/jars\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных `jar` находятся библиотеки, включая классы драйвера и воркеров:\n",
    "- org.apache.spark.repl.Main\n",
    "- org.apache.spark.deploy.SparkSubmit\n",
    "- org.apache.spark.executor.CoarseGrainedExecutorBackend\n",
    "\n",
    "```scala\n",
    "object SparkSubmit extends CommandLineUtils with Logging {\n",
    "  override def main(args: Array[String]): Unit\n",
    "}\n",
    "```\n",
    "\n",
    "```scala\n",
    "private[spark] object CoarseGrainedExecutorBackend extends Logging {\n",
    "    def main(args: Array[String])\n",
    "}\n",
    "```\n",
    "\n",
    "```scala\n",
    "object Main extends Logging {\n",
    "    def main(args: Array[String]): Unit\n",
    "}\n",
    "```\n",
    "\n",
    "Также в дистрибутив входят библиотеки для работы с Hadoop, встроенными источниками данных, например Parquet, и т. д.\n",
    "\n",
    "В каталоге `conf` находятся шаблоны конфигурационных файлов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/conf\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YarnScheulerBackend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spark-defaults.conf**  \n",
    "Основной конфигурационный файл Spark. В нем указываются опции запуска, зависимости, количество воркеров и т. п.\n",
    "\n",
    "**spark-env.sh**  \n",
    "Скрипт, в котором устанавливаются переменные окружения (например, `HADOOP_CONF_DIR` или `YARN_CONF_DIR`)\n",
    "\n",
    "**log4j.properties**\n",
    "Конфигурация логгеров - здесь можно задать необходимый уровень логирования для различных компонентов Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим `spark-submit --help`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"lib/spark-2.4.5-bin-hadoop2.7/bin/spark-submit --help\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приоритет опций (по уменьшению приоритета):\n",
    "- параметры `spark-submit`\n",
    "- переменные окружения\n",
    "- конфигурационный файл `spark-defaults.conf`\n",
    "\n",
    "### Выводы:\n",
    "- Наличие Hadoop не является необходимым условием для работы Spark\n",
    "- В Spark есть большое количество параметров, определяющих режим его работы. Большинство описано здесь: https://spark.apache.org/docs/latest/configuration.html\n",
    "- Утилита `spark-submit` позволяет запустить Spark приложение как локально, так и на кластере\n",
    "\n",
    "## Работа со Spark UI\n",
    "\n",
    "Каждое Spark приложение по умолчанию поднимает UI, который позволяет изучить состояние задачи и провести диагностику производительности. Данные, представленные в UI, также можно получить через REST API\n",
    "\n",
    "Получить Spark UI URL можно следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkUiUrl = Some(http://192.168.88.241:4040)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Some(http://192.168.88.241:4040)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkUiUrl: Option[String] = spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ {\n",
      "  \"id\" : \"local-1634749158085\",\n",
      "  \"name\" : \"Apache Toree\",\n",
      "  \"attempts\" : [ {\n",
      "    \"startTime\" : \"2021-10-20T16:59:16.972GMT\",\n",
      "    \"endTime\" : \"1969-12-31T23:59:59.999GMT\",\n",
      "    \"lastUpdated\" : \"2021-10-20T16:59:16.972GMT\",\n",
      "    \"duration\" : 0,\n",
      "    \"sparkUser\" : \"t3nq\",\n",
      "    \"completed\" : false,\n",
      "    \"appSparkVersion\" : \"2.4.5\",\n",
      "    \"startTimeEpoch\" : 1634749156972,\n",
      "    \"endTimeEpoch\" : -1,\n",
      "    \"lastUpdatedEpoch\" : 1634749156972\n",
      "  } ]\n",
      "} ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys.process._\n",
    "sparkUiUrl.foreach( x => println(s\"curl -s $x/api/v1/applications\".!!))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "appId = local-1634749158085\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "local-1634749158085"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appId = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\" : \"local-1634749158085\",\n",
      "  \"name\" : \"Apache Toree\",\n",
      "  \"attempts\" : [ {\n",
      "    \"startTime\" : \"2021-10-20T16:59:16.972GMT\",\n",
      "    \"endTime\" : \"1969-12-31T23:59:59.999GMT\",\n",
      "    \"lastUpdated\" : \"2021-10-20T16:59:16.972GMT\",\n",
      "    \"duration\" : 0,\n",
      "    \"sparkUser\" : \"t3nq\",\n",
      "    \"completed\" : false,\n",
      "    \"appSparkVersion\" : \"2.4.5\",\n",
      "    \"startTimeEpoch\" : 1634749156972,\n",
      "    \"endTimeEpoch\" : -1,\n",
      "    \"lastUpdatedEpoch\" : 1634749156972\n",
      "  } ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkUiUrl.foreach( x => println(s\"curl -s $x/api/v1/applications/$appId\".!!))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список доступных методов доступен по ссылке: https://spark.apache.org/docs/latest/monitoring.html#rest-api\n",
    "\n",
    "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вредные советы\n",
    "В данной секции приведены частые ошибки, которые допускаются при работе с данными в DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Эта нормальная :)\n",
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [iso_country: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = airports.groupBy('iso_country).count\n",
    "data.cache()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[iso_country: string, count: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Window funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+-------------+-----------------------------------+\n",
      "|ident|iso_country|elevation_ft|type         |count(1) OVER (unspecifiedframe$())|\n",
      "+-----+-----------+------------+-------------+-----------------------------------+\n",
      "|00A  |US         |11          |heliport     |56226                              |\n",
      "|00AA |US         |3435        |small_airport|56226                              |\n",
      "|00AK |US         |450         |small_airport|56226                              |\n",
      "|00AL |US         |820         |small_airport|56226                              |\n",
      "|00AR |US         |237         |closed       |56226                              |\n",
      "|00AS |US         |1100        |small_airport|56226                              |\n",
      "|00AZ |US         |3810        |small_airport|56226                              |\n",
      "|00CA |US         |3038        |small_airport|56226                              |\n",
      "|00CL |US         |87          |small_airport|56226                              |\n",
      "|00CN |US         |3350        |heliport     |56226                              |\n",
      "|00CO |US         |4830        |closed       |56226                              |\n",
      "|00FA |US         |53          |small_airport|56226                              |\n",
      "|00FD |US         |25          |heliport     |56226                              |\n",
      "|00FL |US         |35          |small_airport|56226                              |\n",
      "|00GA |US         |700         |small_airport|56226                              |\n",
      "|00GE |US         |957         |heliport     |56226                              |\n",
      "|00HI |US         |43          |heliport     |56226                              |\n",
      "|00ID |US         |2064        |small_airport|56226                              |\n",
      "|00IG |US         |3359        |small_airport|56226                              |\n",
      "|00II |US         |600         |heliport     |56226                              |\n",
      "+-----+-----------+------------+-------------+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "Window [count(1) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS count(1) OVER (unspecifiedframe$())#242L]\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) Project [ident#167, iso_country#172, elevation_ft#170, type#168]\n",
      "      +- *(1) FileScan csv [ident#167,type#168,elevation_ft#170,iso_country#172] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,elevation_ft:int,iso_country:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wnd = org.apache.spark.sql.expressions.WindowSpec@449f17c7\n",
       "ranked = [ident: string, iso_country: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, iso_country: string ... 3 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val wnd = Window.partitionBy()\n",
    "\n",
    "val ranked = airports.select('ident, 'iso_country, 'elevation_ft, 'type, count(\"*\").over(wnd))\n",
    "ranked.show(20, false)\n",
    "ranked.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+-------------+-----+\n",
      "|ident|iso_country|elevation_ft|type         |count|\n",
      "+-----+-----------+------------+-------------+-----+\n",
      "|00A  |US         |11          |heliport     |56226|\n",
      "|00AA |US         |3435        |small_airport|56226|\n",
      "|00AK |US         |450         |small_airport|56226|\n",
      "|00AL |US         |820         |small_airport|56226|\n",
      "|00AR |US         |237         |closed       |56226|\n",
      "|00AS |US         |1100        |small_airport|56226|\n",
      "|00AZ |US         |3810        |small_airport|56226|\n",
      "|00CA |US         |3038        |small_airport|56226|\n",
      "|00CL |US         |87          |small_airport|56226|\n",
      "|00CN |US         |3350        |heliport     |56226|\n",
      "|00CO |US         |4830        |closed       |56226|\n",
      "|00FA |US         |53          |small_airport|56226|\n",
      "|00FD |US         |25          |heliport     |56226|\n",
      "|00FL |US         |35          |small_airport|56226|\n",
      "|00GA |US         |700         |small_airport|56226|\n",
      "|00GE |US         |957         |heliport     |56226|\n",
      "|00HI |US         |43          |heliport     |56226|\n",
      "|00ID |US         |2064        |small_airport|56226|\n",
      "|00IG |US         |3359        |small_airport|56226|\n",
      "|00II |US         |600         |heliport     |56226|\n",
      "+-----+-----------+------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ident#167, iso_country#172, elevation_ft#170, type#168, 56226 AS count#287L]\n",
      "+- *(1) FileScan csv [ident#167,type#168,elevation_ft#170,iso_country#172] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,elevation_ft:int,iso_country:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count = 56226\n",
       "ranked = [ident: string, iso_country: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, iso_country: string ... 3 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// val wnd = Window.partitionBy()\n",
    "val count = airports.count\n",
    "\n",
    "val ranked = airports.select('ident, 'iso_country, 'elevation_ft, 'type, lit(count).alias(\"count\"))\n",
    "ranked.show(20, false)\n",
    "ranked.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wnd = org.apache.spark.sql.expressions.WindowSpec@2d6faf1a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@2d6faf1a"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wnd = Window.partitionBy(col(\"one\")).orderBy(\"elevation_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printItemPerPartition: [T](ds: org.apache.spark.sql.Dataset[T])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "def printItemPerPartition[T](ds: Dataset[T]): Unit = {\n",
    "    ds.mapPartitions { x => Iterator(x.length) }\n",
    "    .withColumnRenamed(\"value\", \"itemPerPartition\")\n",
    "    .show(300, false)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                  43|56226|\n",
      "+--------------------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[_nondeterministic#1397], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(_nondeterministic#1397, 200)\n",
      "   +- *(1) HashAggregate(keys=[_nondeterministic#1397], functions=[partial_count(1)])\n",
      "      +- *(1) Project [SPARK_PARTITION_ID() AS _nondeterministic#1397]\n",
      "         +- InMemoryTableScan [ident#167, iso_country#172, elevation_ft#170, type#168, row_number() OVER (PARTITION BY one ORDER BY elevation_ft ASC NULLS FIRST unspecifiedframe$())#1269]\n",
      "               +- InMemoryRelation [ident#167, iso_country#172, elevation_ft#170, type#168, row_number() OVER (PARTITION BY one ORDER BY elevation_ft ASC NULLS FIRST unspecifiedframe$())#1269], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- Window [row_number() windowspecdefinition(1, elevation_ft#170 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number() OVER (PARTITION BY one ORDER BY elevation_ft ASC NULLS FIRST unspecifiedframe$())#1269], [1], [elevation_ft#170 ASC NULLS FIRST]\n",
      "                        +- *(2) Sort [1 ASC NULLS FIRST, elevation_ft#170 ASC NULLS FIRST], false, 0\n",
      "                           +- Exchange hashpartitioning(1, 200)\n",
      "                              +- *(1) Project [ident#167, iso_country#172, elevation_ft#170, type#168]\n",
      "                                 +- *(1) FileScan csv [ident#167,type#168,elevation_ft#170,iso_country#172] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,elevation_ft:int,iso_country:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ranked = [ident: string, iso_country: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, iso_country: string ... 3 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val ranked = airports.withColumn(\"one\", lit(1))\n",
    "                .select('ident, 'iso_country, 'elevation_ft, 'type, row_number().over(wnd))\n",
    "\n",
    "ranked.cache\n",
    "ranked.count\n",
    "\n",
    "ranked.groupBy(spark_partition_id()).count.show\n",
    "ranked.groupBy(spark_partition_id()).count.explain\n",
    "// printItemPerPartition(ranked)\n",
    "// ranked.explain\n",
    "ranked.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Distribution\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Distribution(id: Int, count: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(Distribution(1,17662), Distribution(0,38564))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = [ident: string, iso_country: string ... 2 more fields]\n",
       "counts = List(Distribution(1,17662), Distribution(0,38564))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(Distribution(1,17662), Distribution(0,38564))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = airports.select('ident, 'iso_country, 'elevation_ft, 'type)\n",
    "\n",
    "val counts = data.groupBy(spark_partition_id().alias(\"id\")).count.as[Distribution].collect.toList\n",
    "println(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+-------------+---+------------------------+\n",
      "|ident|iso_country|elevation_ft|type         |pid|bar                     |\n",
      "+-----+-----------+------------+-------------+---+------------------------+\n",
      "|00A  |US         |11          |heliport     |0  |[[1, 17662], [0, 38564]]|\n",
      "|00AA |US         |3435        |small_airport|0  |[[1, 17662], [0, 38564]]|\n",
      "+-----+-----------+------------+-------------+---+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "arr_column = array(named_struct(id, 1 AS `id`, count, 17662 AS `count`) AS `foo`, named_struct(id, 0 AS `id`, count, 38564 AS `count`) AS `foo`)\n",
       "zoom = [ident: string, iso_country: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, iso_country: string ... 4 more fields]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val arr_column = array(counts.map(x => struct(lit(x.id).alias(\"id\"), lit(x.count).alias(\"count\")).alias(\"foo\")):_*)\n",
    "\n",
    "val zoom = data.withColumn(\"pid\", spark_partition_id()).withColumn(\"bar\", arr_column)\n",
    "zoom.show(2, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Result\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Result(start: Long, count: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_udf = UserDefinedFunction(<function2>,StructType(StructField(start,LongType,false), StructField(count,LongType,false)),None)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function2>,StructType(StructField(start,LongType,false), StructField(count,LongType,false)),None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val calculate_udf = udf { (pid: Int, bar: Seq[Row]) => \n",
    "    val d = bar.map { row =>\n",
    "        val id = row.getAs[Int](\"id\")\n",
    "        val count = row.getAs[Long](\"count\")\n",
    "        Distribution(id, count)\n",
    "    }.toList\n",
    "    val start = d.filter(x => x.id < pid).map(x => x.count).sum\n",
    "    val thisDistro = d.find(x => x.id == pid).get\n",
    "    Result(start, thisDistro.count)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|value                                      |\n",
      "+-------------------------------------------+\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "|{\"UDF(pid, bar)\":{\"start\":0,\"count\":38564}}|\n",
      "+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zoom.select(calculate_udf('pid, 'bar)).toJSON.show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [height#1777 ASC NULLS FIRST], false, 0\n",
      "+- Exchange RoundRobinPartitioning(1)\n",
      "   +- *(2) HashAggregate(keys=[iso_country#172], functions=[max(elevation_ft#170)])\n",
      "      +- Exchange hashpartitioning(iso_country#172, 200)\n",
      "         +- *(1) HashAggregate(keys=[iso_country#172], functions=[partial_max(elevation_ft#170)])\n",
      "            +- *(1) FileScan csv [elevation_ft#170,iso_country#172] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<elevation_ft:int,iso_country:string>\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ret = [iso_country: string, height: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, height: int]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ret = \n",
    "    airports.groupBy('iso_country).agg(max('elevation_ft).alias(\"height\")).repartition(1)\n",
    "            .sortWithinPartitions('height.asc)\n",
    "ret.write.mode(\"ignore\").parquet(\"/tmp/datasets/out\")\n",
    "ret.explain\n",
    "println(ret.rdd.getNumPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|{\"ident\":\"00A\",\"type\":\"heliport\",\"name\":\"Total ...|\n",
      "|{\"ident\":\"00AA\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AK\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AL\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AR\",\"type\":\"closed\",\"name\":\"Newport...|\n",
      "|{\"ident\":\"00AS\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AZ\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00CA\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00CL\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00CN\",\"type\":\"heliport\",\"name\":\"Kitch...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#1791]\n",
      "+- MapPartitions <function1>, obj#1790: java.lang.String\n",
      "   +- DeserializeToObject createexternalrow(ident#167.toString, type#168.toString, name#169.toString, elevation_ft#170, continent#171.toString, iso_country#172.toString, iso_region#173.toString, municipality#174.toString, gps_code#175.toString, iata_code#176.toString, local_code#177.toString, coordinates#178.toString, StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,true), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true)), obj#1789: org.apache.spark.sql.Row\n",
      "      +- *(1) FileScan csv [ident#167,type#168,name#169,elevation_ft#170,continent#171,iso_country#172,iso_region#173,municipality#174,gps_code#175,iata_code#176,local_code#177,coordinates#178] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsoned = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsoned = airports.toJSON\n",
    "jsoned.show(10, 50)\n",
    "jsoned.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Project [structstojson(named_struct(ident, ident#167, type, type#168, name, name#169, elevation_ft, elevation_ft#170, continent, continent#171, iso_country, iso_country#172, iso_region, iso_region#173, municipality, municipality#174, gps_code, gps_code#175, iata_code, iata_code#176, local_code, local_code#177, coordinates, coordinates#178), Some(Europe/Moscow)) AS structstojson(named_struct(NamePlaceholder(), unresolvedstar()))#1809]\n",
      "+- *(1) FileScan csv [ident#167,type#168,name#169,elevation_ft#170,continent#171,iso_country#172,iso_region#173,municipality#174,gps_code#175,iata_code#176,local_code#177,coordinates#178] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n"
     ]
    }
   ],
   "source": [
    "airports.select(to_json(struct(col(\"*\")))).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Apple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Apple(size: Int, color: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [input[0, int, false] AS value#1820]\n",
      "+- *(1) MapElements <function1>, obj#1819: int\n",
      "   +- *(1) DeserializeToObject newInstance(class $line255.$read$$iw$$iw$Apple), obj#1818: $line255.$read$$iw$$iw$Apple\n",
      "      +- LocalTableScan [size#1814, color#1815]\n"
     ]
    }
   ],
   "source": [
    "List(Apple(1, \"red\")).toDS.map(x => x.size).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [size#1837]\n",
      "+- Scan ExistingRDD[size#1837,color#1838]\n"
     ]
    }
   ],
   "source": [
    "List(Apple(1, \"red\")).toDS.localCheckpoint.select('size).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.InternalRow\n",
    "java.lang.Long // != Long\n",
    "java.lang.Integer // != Int\n",
    "java.lang.String // == String (UTF8String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mega_udf = UserDefinedFunction(<function2>,StringType,Some(List(IntegerType, StringType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function2>,StringType,Some(List(IntegerType, StringType)))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mega_udf = udf { (left: java.lang.Integer, right: String) => \"ok\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|UDF(left, right)|\n",
      "+----------------+\n",
      "|              ok|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        lit(1).alias(\"left\"), \n",
    "        lit(\"foo\").alias(\"right\")\n",
    "    )\n",
    "    .select(mega_udf('left, 'right))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|UDF(left, right)|\n",
      "+----------------+\n",
      "|              ok|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        lit(null).alias(\"left\"), \n",
    "        lit(\"foo\").alias(\"right\")\n",
    "    )\n",
    "    .select(mega_udf('left, 'right))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|UDF(left, right)|\n",
      "+----------------+\n",
      "|              ok|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        lit(1).alias(\"left\"), \n",
    "        lit(null).alias(\"right\")\n",
    "    )\n",
    "    .select(mega_udf('left, 'right))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.NullPointerException\n",
       "Message: null\n",
       "StackTrace:   at scala.Predef$.Integer2int(Predef.scala:362)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: java.lang.Integer = null\n",
    "val foe: Int = if (foo == null) ??? else foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Foo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Foo(first: Int, second: Int, third: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mega_udf2 = UserDefinedFunction(<function0>,StructType(StructField(first,IntegerType,false), StructField(second,IntegerType,false), StructField(third,IntegerType,false)),Some(List()))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function0>,StructType(StructField(first,IntegerType,false), StructField(second,IntegerType,false), StructField(third,IntegerType,false)),Some(List()))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mega_udf2 = udf { () => Thread.sleep(1000); Foo(1,2,3) }.asNondeterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+\n",
      "|res.first|res.second|res.third|\n",
      "+---------+----------+---------+\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "|        1|         2|        3|\n",
      "+---------+----------+---------+\n",
      "\n",
      "Time taken: 10135 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "spark\n",
    "    .range(0, 10, 1, 1)\n",
    "    .select(mega_udf2().alias(\"res\"))\n",
    "    .select('res(\"first\"), 'res(\"second\"), 'res(\"third\")).show\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time { \n",
    "spark\n",
    "    .range(0, 10, 1, 1)\n",
    "    .select(mega_udf2().alias(\"res\"))\n",
    "    .select(col(\"res.*\")).show\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('res[first], None), unresolvedalias('res[second], None), unresolvedalias('res[third], None)]\n",
      "+- Project [UDF() AS res#1987]\n",
      "   +- Range (0, 10, step=1, splits=Some(1))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "res.first: int, res.second: int, res.third: int\n",
      "Project [res#1987.first AS res.first#1996, res#1987.second AS res.second#1997, res#1987.third AS res.third#1998]\n",
      "+- Project [UDF() AS res#1987]\n",
      "   +- Range (0, 10, step=1, splits=Some(1))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [res#1987.first AS res.first#1996, res#1987.second AS res.second#1997, res#1987.third AS res.third#1998]\n",
      "+- Project [UDF() AS res#1987]\n",
      "   +- Range (0, 10, step=1, splits=Some(1))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [res#1987.first AS res.first#1996, res#1987.second AS res.second#1997, res#1987.third AS res.third#1998]\n",
      "+- *(1) Project [UDF() AS res#1987]\n",
      "   +- *(1) Range (0, 10, step=1, splits=1)\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "    .range(0, 10, 1, 1)\n",
    "    .select(mega_udf2().alias(\"res\"))\n",
    "    .select('res(\"first\"), 'res(\"second\"), 'res(\"third\")).explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"\"\"cp -f /tmp/datasets/source/1.txt /tmp/datasets/cache.txt/1.txt\"\"\".!\n",
    "\"\"\"cp -f /tmp/datasets/source/2.txt /tmp/datasets/cache.txt/2.txt\"\"\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.text(\"/tmp/datasets/cache.txt\")\n",
    "// df.show(10)\n",
    "\n",
    "// df.cache\n",
    "df.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.localCheckpoint\n",
    "df.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"cp -f /tmp/datasets/source/1.txt /tmp/datasets/cache.txt/2.txt\"\"\".!\n",
    "\"\"\"cp -f /tmp/datasets/source/3.txt /tmp/datasets/cache.txt/1.txt\"\"\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(\"/tmp/datasets/cache.txt\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sharedState.cacheManager.clearCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Coalesce 1\n",
      "+- *(1) Project [id#2185L, UDF() AS foo#2187]\n",
      "   +- *(1) Range (0, 10, step=1, splits=2)\n",
      "Time taken: 10108 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time {\n",
    "val ret = spark.range(0, 10, 1, 2).withColumn(\"foo\", mega_udf2()).coalesce(1)\n",
    "ret.collect\n",
    "ret.explain\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Coalesce 1\n",
      "+- InMemoryTableScan [id#2192L, foo#2194]\n",
      "      +- InMemoryRelation [id#2192L, foo#2194], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) Project [id#2192L, UDF() AS foo#2194]\n",
      "               +- *(1) Range (0, 10, step=1, splits=2)\n",
      "Time taken: 5163 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time {\n",
    "val ret = spark.range(0, 10, 1, 2).withColumn(\"foo\", mega_udf2())\n",
    "ret.cache\n",
    "ret.count\n",
    "ret.coalesce(1).collect\n",
    "ret.coalesce(1).explain\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sharedState.cacheManager.clearCache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
