# Lab04b. Агрегация данных из потока
Для приложения аналитики онлайн-магазина поступают данные о визитах пользователя на странички товаров (клики) и о покупках. Вы напишите приложение, которое можно использовать и для агрегации данных за прошедший день, и для агрегации в режиме (псевдо)реального времени (с выдачей результата по завершении каждого временного периода). Это так называемая Kappa-архитектура, а для реализации мы используем `Spark Structured Streaming`.

На верхнем уровне пайплайн будет выглядеть так: Kafka —> Spark Streaming —> Kafka.

## I. Задача с высоты птичьего полета

Вам нужно:

1. Получать события из топика Kafka `name_surname` через Spark Structured Streaming. Данные приходят в виде, описанном в Lab04a.

2. Подсчитать следующие метрики за каждый час:
   1. общая сумма продаж,
   2. число посетителей,
   3. число заказов,
   4. средний чек.
   
3. Отправить данные в новый топик Kafka `name_surname_lab04b_out`.

![Alt text](images/img4b.png?raw=true "Архитектура")

## II. Описание данных

Сообщения в Kafka - те же самые, как и Lab04a.

## III. Методика расчета метрик

1. **Общая сумма продаж (`revenue`) за период**

Сумма всех `item_price`, для которых была совершена покупка в этот период. Факт покупки отражен в поле `"event_type": "buy"`.

2. **Число идентифицированных посещений (`visitors`) за период**

Количество посещений, у которых `uid` не `null` в этот период.

3. **Число покупок (`purchases`) за период**

Количество `"event_type": "buy"`, в этот период. 

4. **Средний чек (`aov`) за период**

Это `revenue`, деленное на `purchases`. 

Первый временной период определяется самым ранним `timestamp` в потоке данных + 1 час. Последующие временные периоды идут с интервалом в 1 час вплоть до конца потока данных. Использовать `timestamp` из данных, приходящих в Kafka.

Размер микробатча данных, получаемых из Kafka, определяется временным промежутком, равным `5 секундам`.

Запись данных в результирующую таблицу происходит в режиме `Update`. `Append` — не подходит, потому что в следующем микро-батче могут прийти данные, касающиеся уже посчитанных данных по одному из временных интервалов. `Complete` — будет избыточным, потому что чем дальше, тем меньше вероятность изменения данных, уже содержащихся в результирующей таблице.

Записи в итоговом топике в Kafka должны выглядеть следующим образом:

```json
{"start_ts":1577865600,"end_ts":1577869200,"revenue":80271,"visitors":46,"purchases":28,"aov":2866.8214285714284}
{"start_ts":1577869200,"end_ts":1577872800,"revenue":96379,"visitors":32,"purchases":40,"aov":2409.475}
...
```

## III. Оформление работы

В вашем репо в подпапке `lab04b/agg` положите sbt-project под названием agg с главным классом agg в файле agg.scala.

Проект должен компилироваться и запускаться следующим образом:

```
cd lab04b/agg
sbt package
spark-submit --class agg --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 ./target/scala-2.11/agg_2.11-1.0.jar 
```

## IV. Проверка

Запустите ваш проект, дождитесь полной загрузки приложения (в данной работе чекер не запускает Ваш проект).

Запусите чекер на [странице в ЛК](https://lk-spark-de.newprolab.com/lab/sb1labb04). В данном случае чекер выполняет следующие действия:

- отправляет в топик Kafka `name_surname` тестовый набор данных,
- читает данные с топика `name_surname_lab04b_out`,
- проверяет правильность расчета данных по метрикам.
- логи чекера доступны по пути /tmp/logs/labname/name.surname

### Описание полей чекера:

* `info_kafka_errors` – возможные ошибки Kafka,
* `total_number_correct` – True/False: совпадает ли общее количество записей с ответом (в случае False остальные поля чекера не рассчитываются),
* `info_number_sent` – количество посланных записей,
* `info_number_recieved` – количество полученных записей,
* `info_number_of_correct_records` – количество правильных записей, при условии что общее количество записей правильное,
* `info_wrong_fields` – список полей, где есть неправильные значения,
* `metrics_correct` – True/False: правильно ли посчитаны все метрики,
* `lab_result` – True/False: общий результат лабы.

## IV. Подсказки

- Удаляйте чекпойнты перед каждым запуском!
- Убедитесь, что топик пуст перед запуском чекера.
- Иногда на запуске чекера получается `info_number_recieved = 0`. Просто запустите чекер снова.
- Очистка топиков происходит примерно каждые 15 минут
- Проверить пуст ли топик: `/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --bootstrap-server spark-master-1:6667 --topic topic_name --from-beginning`
> Для чекера необходимо, чтобы итоговые записи получились без дублей по времени. Для этого необходимо, чтобы в один тик ProcessingTime успели прочитаться все записи из Кафки. 

