{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val USER = \"***\"\n",
    "val HDFS_DIR = s\"/user/$USER/visits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.users_items.update\", 0)\n",
    "spark.conf.set(\"spark.users_items.input_dir\", s\"/user/$USER/visits\")\n",
    "spark.conf.set(\"spark.users_items.output_dir\", s\"/user/$USER/users-items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf_update = spark.conf.get(\"spark.users_items.update\")\n",
    "val conf_input_dir = spark.conf.get(\"spark.users_items.input_dir\")\n",
    "val conf_output_dir = spark.conf.get(\"spark.users_items.output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "union_cols: (myCols: Set[String], allCols: Set[String])List[org.apache.spark.sql.Column]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def union_cols(myCols: Set[String], allCols: Set[String]) = {\n",
    "    allCols.toList.map( x => x\n",
    "        match {\n",
    "            case x if myCols.contains(x) => col(x)\n",
    "            case _ => lit(0).as(x)\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [category: string, date: string ... 5 more fields]\n",
       "MAX_DATE = 20200429\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20200429"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read\n",
    "    .option(\"header\", true)\n",
    "    .json(HDFS_DIR + \"/*/*/*.json\")\n",
    "    .toDF\n",
    "\n",
    "val MAX_DATE = data.select(date_format(max(('timestamp / 1000).cast(\"timestamp\")), \"yyyyMMdd\")).collect()(0)(0).toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da ya tut\n"
     ]
    }
   ],
   "source": [
    "if (conf_update.equals(\"0\"))\n",
    "{    \n",
    "    val dataTransformed = data\n",
    "        .select('uid, 'event_type, 'item_id)\n",
    "        .filter('uid.isNotNull)\n",
    "        .withColumn(\"item\",\n",
    "            lower(concat(\n",
    "                concat('event_type, lit(\"_\")),  regexp_replace('item_id, \"[ -]\", \"_\"))))\n",
    "        .drop(\"event_type\", \"item_id\")\n",
    "        .groupBy(\"uid\", \"item\").count\n",
    "    \n",
    "    val dataMatrix = dataTransformed\n",
    "        .groupBy(\"uid\")\n",
    "        .pivot(\"item\")\n",
    "        .sum(\"count\")\n",
    "        .na.fill(0)\n",
    "    \n",
    "    dataMatrix\n",
    "        .write\n",
    "        .format(\"parquet\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(conf_output_dir + s\"/${MAX_DATE}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (conf_update.equals(\"1\"))\n",
    "{\n",
    "    val OLD_MAX_DATE = MAX_DATE\n",
    "\n",
    "    val oldMatrix = spark.read.parquet(s\"${conf_output_dir}/${OLD_MAX_DATE}/*\")\n",
    "\n",
    "    val newData = spark.read\n",
    "        .option(\"header\", true)\n",
    "        .json(conf_input_dir + \"/*/*/*.json\")\n",
    "        .toDF\n",
    "\n",
    "    val NEW_MAX_DATE = newData.select(date_format(max(('timestamp / 1000).cast(\"timestamp\")), \"yyyyMMdd\")).collect()(0)(0).toString\n",
    "\n",
    "    val newMatrix = newData\n",
    "        .select('uid, 'event_type, 'item_id)\n",
    "        .filter('uid.isNotNull)\n",
    "        .withColumn(\"item\",\n",
    "            lower(concat(\n",
    "                concat('event_type, lit(\"_\")),  regexp_replace('item_id, \"[ -]\", \"_\"))))\n",
    "        .drop(\"event_type\", \"item_id\")\n",
    "        .groupBy(\"uid\", \"item\")\n",
    "        .count\n",
    "        .groupBy(\"uid\")\n",
    "        .pivot(\"item\")\n",
    "        .sum(\"count\")\n",
    "        .na.fill(0)\n",
    "\n",
    "    val oldCols = oldMatrix.columns.toSet\n",
    "    val newCols = newMatrix.columns.toSet\n",
    "    val total = oldCols ++ newCols\n",
    "    \n",
    "    val resMatrix = oldMatrix.select(union_cols(oldCols, total):_*).union(newMatrix.select(union_cols(newCols, total):_*))\n",
    "    \n",
    "    resMatrix\n",
    "        .write\n",
    "        .format(\"parquet\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(conf_output_dir + s\"/${NEW_MAX_DATE}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "//spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
