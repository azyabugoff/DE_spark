# Lab07. Обучение и инференс, используя Spark ML

В предыдущей лабе вы подготовили данные для обучения - сгенерировали некоторое количество признаков. В этой лабе вы продолжите работать в рамках проекта, и вам теперь нужно будет обучить простую модель классификации и сделать предсказания на новых данных.
Для того, что бы не загружать вас МL-инжинирингом, базовая версия лабораторной работы использует только стандартные классы библиотеки Spark ML. Но для тех, кто не против погрузится в тему обеими руками вместе с головой, мы подготовили Lab07s, в которой предлагается реализовать собственные классы трансформера и эстиматора, причем в последнем вы будете использовать модель на питоне.

## I. Задача с высоты птичьего полёта

Обучите модель `LogisticRegression` на датасете weblogs, используя SparkML и Pipeline. 

Разработайте приложение на Spark для инференса тестового датасета из Kafka, используя Spark Structured Streaming.

![Alt text](images/img7.png?raw=true "Архитектура")

## II. Описание данных

*Обучающая выборка*, с которой вы будете работать, выглядит следующим образом (знакомый вам датасет `weblogs` (путь на hdfs: /labs/laba07/laba07.json), в котором добавлены метки класса для обучения).

```json
{
  "uid": "d50192e5-c44e-4ae8-ae7a-7cfe67c8b777",
  "gender_age": "F:18-24",
  "visits": [
    {
      "url": "http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun",
      "timestamp": 1419688144068
    },
    {
      "url": "http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story",
      "timestamp": 1426666298001
    },
    {
      "url": "http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html",
      "timestamp": 1426666298000
    },
    {
      "url": "http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story",
      "timestamp": 1426661722001
    },
    {
      "url": "http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html",
      "timestamp": 1426661722000
    }
  ]
}
```
Поле `uid` принимает значения уникального ID пользователя (cookies): `d50192e5-c44e-4ae8-ae7a-7cfe67c8b777`.

Поле `gender_age` – это составная метка класса пользователя. Пол принимает значения `F` (женщина) и `M` (мужчина), возраст принимает значения диапазонов: `18-24`, `25-34`, `35-44`, `45-54`, `>=55`. Таким образом получается 2x5=10 классов пользователей по половозрастному признаку.

Поле `visits` это json со следующей схемой данных: `{"visits": [{"url": "url1", "timestamp": "timestamp1"}, {"url": "url2", "timestamp": "timestamp2"}, ... ]}`. В нем содержатся непосредственно URL посещенных пользователем страниц вместе с временной меткой посещения. Число визитов – произвольное, варьируется от юзера к юзеру. 

Этот датасет находится в директории `hdfs:///labs/laba07`):

Данные, по которым вам надо будет построить прогноз (*тестовая выборка*), будут прилетать в ваш персональный топик Kafka name_surname в таком виде, по нажатию на кнопку чекера:

```json
{
  "uid": "bd7a30e1-a25d-4cbf-a03f-61748cbe540e",
  "visits": [
    {
      "url": "http://www.interfax.ru/business/414668",
      "timestamp": 1419775945781
    },
    {
      "url": "http://amerikan-gruzovik.ru/zapchasti-dlya-amerikanskikh-gruzovikov.html",
      "timestamp": 1419679865088
    }
  ]
}
```

То есть все то же самое, только без поля `gender_age`. 

## III. Оформление работы для решения со SparkML и Pipeline.

Перед началом создайте выходной топик в Kafka при помощи команды:
/usr/hdp/current/kafka-broker/bin/kafka-topics.sh  --create --topic name_surname_lab07_out --zookeeper spark-node-1.newprolab.com:2181 --partitions 1 --replication-factor 1

В вашем репо в подпапке `lab07/mlproject` положите sbt-project под названием `mlproject`. В вашем проекте должны быть отдельно два класса: для обучения (`class train` в файле `train.scala`) и инференса (`class test` в файле `test.scala`) модели.

Проект должен компилироваться и запускаться следующим образом:

```
cd lab07/mlproject
sbt package
# тренировка
spark-submit --class train .target/scala-2.11/mlproject_2.11-1.0.jar 
# инференс
spark-submit --class test --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 .target/scala-2.11/mlproject_2.11-1.0.jar 
```

*train.scala* содержит обучение модели. В нем не должно быть никаких hard-coded значений – все должно извлекаться из файла модели, либо из конфиг-параметров `spark-submit`. 

Программа *train.scala* должна принимать следующие аргументы через spark.conf:

* путь к тренировочному датасету (в HDFS)
* путь для сохранения spark-pipeline (в HDFS)

*test.scala* содержит код инференса на стриме из Kafka.

Программа для предсказания `test.scala` для предсказания принимает следующие аргументы:

* путь к сохраненной в HDFS `spark-pipеline`
* топик Kafka, куда придут тестовые данные
* топик Kafka, куда выдавать предсказания

## IV. Модель

В качестве модели возьмите логистическую регрессию с некоторыми параметрами:

```
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.001)
```

Определите пайплайн, в который включите библиотечные трансформеры. Ниже приведена модель, которую можно использовать в качестве минимальной, дабавив код для экстракции домена из URL:

```scala
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{CountVectorizer, StringIndexer, IndexToString}
import org.apache.spark.ml.{Pipeline, PipelineModel}


val cv = new CountVectorizer()
      .setInputCol("domains")
      .setOutputCol("features")

val indexer = new StringIndexer()
      .setInputCol("gender_age")
      .setOutputCol("label")

val lr = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.001)

val pipeline = new Pipeline()
      .setStages(Array(cv, indexer, lr))

```

Эта модель определена в виде конвейера данных (pipeline), приемущество которого в том, что его можно использовать и с тренировочным набором, и с тестовым, а так же сохранять на диск и загружать в другой программе.

В качестве признаков модель принимает колонку с массивом доменов, посещенных пользователем, в качестве меток классов – колонку `gender_age`.

Как видите, модель не использует время посещения сайтов. То есть идея в том, что для понимания на каких доменах побывал человек достаточно для того, чтобы определить его пол и возрастную категорию (правда, как вы убедитесь, с невысокой точностью). Модель машинного обучения сама поймет эту взаимосвязь, нам вручную правила прописывать не нужно будет. 

Примерно вот так выглядит датафрейм, который вы подаете на обучение:

```
+--------------------+-------------------+------------+
|                 uid|            domains|  gender_age|
+--------------------+-------------------+------------+
|d50192e5-c44e-4ae...|['zebra-zoya.ru'...|     F:18-24|
|d502331d-621e-472...|['sweetrading.ru...|     M:25-34|
...
```
> Модель нужно доработать, чтобы была возможность расшифровывать посчитаные Indexes на этапе применения модели. 
> Для этого последним этапом нужно добавить IndexToString и дать ему на вход indexer.labels. Они появятся, если сделать .fit(...) у indexer.


### Обучение 

Обучите пайплайн методом `pipeline.fit()` и сохраните обученный объект `PipelineModel`:

```
val model = pipeline.fit(training)

model.write.overwrite().save(model_path)
```

### Предсказание
Напишите второе приложение, использующее Spark Structured Streaming и Kafka. 

Оно должно: 
* загрузить ранее сохраненную модель
* считывать данные из входного топика Kafka методом readStream
* преобразовывать данные к принимаемому моделью виду (кроме колонки `label_string` — она только для обучения)
* делать предсказание по ним и отправлять в real-time эти предсказания в выходной топик в Kafka в формате:

```json
{"uid": "fe1dba8f-3131-439f-9031-851c0da0f126", "gender_age": "M:25-34"}
{"uid": "d50192e5-c44e-4ae8-ae7a-7cfe67c8b777", "gender_age": "F:18-24"}
```

Название входного топика: `name_surname`, выходного: `name_surname_lab07_out`, где `name_surname` — ваш логин в личном кабинете с подчерком вместо точки. После отправки данных чекер будет ждать 30 секунд, чтобы вы обработали данные из входного топика и записали результаты предсказания уже в выходной топик. 

Код для загрузки и инференса:
```
val model = PipelineModel.load(model_path)
df = model.transform(test)
```

## V. Проверка

Проверка осуществляется автоматическим скриптом на странице лабы в личном кабинете. Наш чекер сначала засылает в ваш входной топик тестовый набор данных, а на выходном топике принимает прогноз и затем сравнивает ваш прогноз с эталонным ответом. На выходе вы получите свой score. В качестве него — метрика `accuracy`: доля пользователей от общего числа, по которым вы верно предсказали и пол, и возрастную категорию. Барьер для прохождения не ставится – любой результат с метрикой выше 0 подойдет. Перед нажатием кнопки проверки необходимо запустить стриминг вручную.

### Поля чекера
- `00_git_correct` – True/False: нужные файлы присутствуют в гите,
- `01_info_git_errors` – что не так с репозиторием, если git_correct=False,
- `02_info_kafka_errors` – ошибки Kafka, в том числе наличие входного топика, наличие выходного топика,
- `03_info_num_records_output_topic` – число, показывает количество записей в топике `name_surname_lab07_out` (должно соответствовать количеству записей в топике `name_surname`),
- `04_total_number_correct` – True/False: проверяет равны ли эти количества друг другу,
- `05_info_accuracy` – число, показывает достигнутое значение метрики,
- `06_accuracy_achieved` – True/False: достигнут или нет требуемый уровень точности модели,
- `07_lab_result` — True/False: зачет или нет.

### Подсказки

* Модель предсказывает число – номер класса, но на выход вам надо отправлять поле `gender_age`. Вам надо преобразовать номер класса в `gender_age`. Метки можно  конвертировать в строки с помощью трансформера `IndexToString` (обратного к `StringIndexer`). Добавьте его в пайплайн самостоятельно.

## Задачи со звездочкой

(проверятся не будут - чисто для своего удовольствия)

* Напишите код для подбора гиперпараметров модели в процессе обучения, используя `ParamGridBuilder` и `CrossValidator`, чтобы улучшить ее результат.
* Можете еще поработать над моделью и применить другие алгоритмы, например. Но это, в основном, для тех, кто знает основы машинного обучения или хочет разобраться.
