{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@200d7e02\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@200d7e02"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"Lab04\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "killAll: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfInput = [key: binary, value: binary ... 5 more fields]\n",
       "df = [revenue: double, visitors: bigint ... 4 more fields]\n",
       "schema = StructType(StructField(category,StringType,true), StructField(event_type,StringType,true), StructField(item_id,StringType,true), StructField(item_price,StringType,true), StructField(timestamp,LongType,true), StructField(uid,StringType,true))\n",
       "df = [revenue: double, visitors: bigint ... 4 more fields]\n",
       "df = [revenue: double, visitors: bigint ... 4 more fields]\n",
       "df = [revenue: double, visitors: bigint ... 4 more fields]\n",
       "df = [revenue: double,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[revenue: double,..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfInput = spark.readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"spark-master-1:6667\")\n",
    "  .option(\"subscribe\", \"lab04_in\")\n",
    "  .load()\n",
    "\n",
    "var df = dfInput.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "  StructField(\"category\", StringType, true),\n",
    "  StructField(\"event_type\", StringType, true),\n",
    "  StructField(\"item_id\", StringType, true),\n",
    "  StructField(\"item_price\", StringType, true),\n",
    "  StructField(\"timestamp\", LongType, true),\n",
    "  StructField(\"uid\", StringType, true)\n",
    "))\n",
    "\n",
    "df = df.withColumn(\"jsonData\", from_json(col(\"value\"), schema)).select(\"jsonData.*\")\n",
    "\n",
    "df = df.withColumn(\"date\", ($\"timestamp\" / 1000).cast(TimestampType))\n",
    "\n",
    "df = df.groupBy(window(col(\"date\"), \"1 hours\"/*, \"5 seconds\"*/)).agg(\n",
    "  sum(when(col(\"event_type\") === \"buy\", col(\"item_price\")).otherwise(0)).alias(\"revenue\"),\n",
    "  sum(when(col(\"uid\").isNotNull, 1).otherwise(0)).alias(\"visitors\"),\n",
    "  sum(when(col(\"event_type\") === \"buy\", 1).otherwise(0)).alias(\"purchases\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"aov\", col(\"revenue\")/col(\"purchases\"))\n",
    "df = df.withColumn(\"start_ts\", col(\"window.start\").cast(\"long\"))\n",
    "df = df.withColumn(\"end_ts\", col(\"window.end\").cast(\"long\"))\n",
    "df = df.drop(col(\"window\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = df\n",
    "  .selectExpr(\"CAST(start_ts AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "  .writeStream\n",
    "  .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
    "  .format(\"kafka\")\n",
    "  .option(\"checkpointLocation\", \"/tmp/chk\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"10.0.0.5:6667\")\n",
    "  .option(\"topic\", \"lab04b_out\")\n",
    "  .option(\"maxOffsetsPerTrigger\", 200)\n",
    "  .outputMode(\"update\")\n",
    "  .start()\n",
    "\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
